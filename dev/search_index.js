var documenterSearchIndex = {"docs":
[{"location":"example/#Extended-Usage-Example","page":"Extended Example","title":"Extended Usage Example","text":"","category":"section"},{"location":"example/","page":"Extended Example","title":"Extended Example","text":"To show you how text analysis might work in practice, we're going to work with a text corpus composed of political speeches from American presidents given as part of the State of the Union Address tradition.","category":"page"},{"location":"example/","page":"Extended Example","title":"Extended Example","text":"    using TextAnalysis, MultivariateStats, Clustering\n\n    crps = DirectoryCorpus(\"sotu\")\n\n    standardize!(crps, StringDocument)\n\n    crps = Corpus(crps[1:30])\n\n    remove_case!(crps)\n    prepare!(crps, strip_punctuation)\n\n    update_lexicon!(crps)\n    update_inverse_index!(crps)\n\n    crps[\"freedom\"]\n\n    m = DocumentTermMatrix(crps)\n\n    D = dtm(m, :dense)\n\n    T = tf_idf(D)\n\n    cl = kmeans(T, 5)","category":"page"},{"location":"semantic/#LSA:-Latent-Semantic-Analysis","page":"Semantic Analysis","title":"LSA: Latent Semantic Analysis","text":"","category":"section"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"Often we want to think about documents from the perspective of semantic content. One standard approach to doing this, is to perform Latent Semantic Analysis or LSA on the corpus.","category":"page"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"lsa","category":"page"},{"location":"semantic/#TextAnalysis.lsa","page":"Semantic Analysis","title":"TextAnalysis.lsa","text":"lsa(dtm::DocumentTermMatrix)\nlsa(crps::Corpus)\n\nPerforms Latent Semantic Analysis or LSA on a corpus.\n\n\n\n\n\n","category":"function"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"lsa uses tf_idf for statistics.","category":"page"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"using TextAnalysis\ncrps = Corpus([\n  StringDocument(\"this is a string document\"),\n  TokenDocument(\"this is a token document\")\n])\nlsa(crps)","category":"page"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"lsa can also be performed on a DocumentTermMatrix.","category":"page"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"using TextAnalysis\ncrps = Corpus([\n  StringDocument(\"this is a string document\"),\n  TokenDocument(\"this is a token document\")\n]);\nupdate_lexicon!(crps)\n\nm = DocumentTermMatrix(crps)\n\nlsa(m)","category":"page"},{"location":"semantic/#LDA:-Latent-Dirichlet-Allocation","page":"Semantic Analysis","title":"LDA: Latent Dirichlet Allocation","text":"","category":"section"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"Another way to get a handle on the semantic content of a corpus is to use Latent Dirichlet Allocation:","category":"page"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"First we need to produce the DocumentTermMatrix","category":"page"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"lda","category":"page"},{"location":"semantic/#TextAnalysis.lda","page":"Semantic Analysis","title":"TextAnalysis.lda","text":"ϕ, θ = lda(dtm::DocumentTermMatrix, ntopics::Int, iterations::Int, α::Float64, β::Float64; kwargs...)\n\nPerform Latent Dirichlet allocation.\n\nRequired Positional Arguments\n\nα Dirichlet dist. hyperparameter for topic distribution per document. α<1 yields a sparse topic mixture for each document. α>1 yields a more uniform topic mixture for each document.\nβ Dirichlet dist. hyperparameter for word distribution per topic. β<1 yields a sparse word mixture for each topic. β>1 yields a more uniform word mixture for each topic.\n\nOptional Keyword Arguments\n\nshowprogress::Bool. Show a progress bar during the Gibbs sampling. Default value: true.\n\nReturn Values\n\nϕ: ntopics × nwords Sparse matrix of probabilities s.t. sum(ϕ, 1) == 1\nθ: ntopics × ndocs Dense matrix of probabilities s.t. sum(θ, 1) == 1\n\n\n\n\n\n","category":"function"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"using TextAnalysis\ncrps = Corpus([\n  StringDocument(\"This is the Foo Bar Document\"),\n  StringDocument(\"This document has too Foo words\")\n]);\nupdate_lexicon!(crps)\nm = DocumentTermMatrix(crps)\n\nk = 2             # number of topics\niterations = 1000 # number of gibbs sampling iterations\nα = 0.1           # hyper parameter\nβ  = 0.1          # hyper parameter\n\nϕ, θ  = lda(m, k, iterations, α, β);\nϕ\nθ","category":"page"},{"location":"semantic/","page":"Semantic Analysis","title":"Semantic Analysis","text":"See ?lda for more help.","category":"page"},{"location":"APIReference/#API-References","page":"API References","title":"API References","text":"","category":"section"},{"location":"APIReference/","page":"API References","title":"API References","text":"Modules = [TextAnalysis]\nOrder   = [:function, :type]","category":"page"},{"location":"APIReference/#Base.argmax-Tuple{Vector{Score}}","page":"API References","title":"Base.argmax","text":"argmax(scores::Vector{Score})::Score\n\nscores - vector of Score\n\nReturns maximum by precision fiels of each Score\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#Base.merge!-Union{Tuple{T}, Tuple{DocumentTermMatrix{T}, DocumentTermMatrix{T}}} where T","page":"API References","title":"Base.merge!","text":"merge!(dtm1::DocumentTermMatrix{T}, dtm2::DocumentTermMatrix{T}) where {T}\n\nMerge one DocumentTermMatrix instance into another. Documents are appended to the end. Terms are re-sorted. For efficiency, this may result in modifications to dtm2 as well.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.DirectoryCorpus-Tuple{AbstractString}","page":"API References","title":"TextAnalysis.DirectoryCorpus","text":"DirectoryCorpus(dirname::AbstractString)\n\nConstruct a Corpus from a directory of text files.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.author!-Tuple{AbstractDocument, AbstractString}","page":"API References","title":"TextAnalysis.author!","text":"author!(doc, author)\n\nSet the author metadata of doc to author.\n\nSee also: author, authors, authors!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.author-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.author","text":"author(doc)\n\nReturn the author metadata for doc.\n\nSee also: author!, authors, authors!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.authors!-Tuple{Corpus, Vector{String}}","page":"API References","title":"TextAnalysis.authors!","text":"authors!(crps, athrs)\nauthors!(crps, athr)\n\nSet the authors of the documents in crps to the athrs, respectively.\n\nSee also: authors, author!, author\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.authors-Tuple{Corpus}","page":"API References","title":"TextAnalysis.authors","text":"authors(crps)\n\nReturn the authors for each document in crps.\n\nSee also: authors!, author, author!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.average-Tuple{Vector{Score}}","page":"API References","title":"TextAnalysis.average","text":"average(scores::Vector{Score})::Score\n\nscores - vector of Score\n\nReturns average values of scores as a Score with precision/recall/fmeasure\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.bleu_score-Union{Tuple{T}, Tuple{Vector{<:T}, T}} where T<:(Vector{<:Vector{<:AbstractString}})","page":"API References","title":"TextAnalysis.bleu_score","text":"bleu_score(reference_corpus::Vector{Vector{Token}}, translation_corpus::Vector{Token}; max_order=4, smooth=false)\n\nComputes BLEU score of translated segments against one or more references. Returns the BLEU score, n-gram precisions, brevity penalty,  geometric mean of n-gram precisions, translationlength and  referencelength\n\nArguments\n\nreference_corpus: list of lists of references for each translation. Each reference should be tokenized into a list of tokens.\ntranslation_corpus: list of translations to score. Each translation should be tokenized into a list of tokens.\nmax_order: maximum n-gram order to use when computing BLEU score. \nsmooth=false: whether or not to apply. Lin et al. 2004 smoothing.\n\nExample:\n\none_doc_references = [\n    [\"apple\", \"is\", \"apple\"],\n    [\"apple\", \"is\", \"a\", \"fruit\"]\n]  \none_doc_translation = [\n    \"apple\", \"is\", \"appl\"\n]\nbleu_score([one_doc_references], [one_doc_translation], smooth=true)\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.columnindices-Union{Tuple{Vector{T}}, Tuple{T}} where T","page":"API References","title":"TextAnalysis.columnindices","text":"columnindices(terms::Vector{String})\n\nCreates a column index lookup dictionary from a vector of terms.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.coo_matrix-Union{Tuple{T}, Tuple{Type{T}, Vector{<:AbstractString}, OrderedCollections.OrderedDict{<:AbstractString, Int64}, Int64}, Tuple{Type{T}, Vector{<:AbstractString}, OrderedCollections.OrderedDict{<:AbstractString, Int64}, Int64, Bool}, Tuple{Type{T}, Vector{<:AbstractString}, OrderedCollections.OrderedDict{<:AbstractString, Int64}, Int64, Bool, Symbol}} where T<:AbstractFloat","page":"API References","title":"TextAnalysis.coo_matrix","text":"coo_matrix(::Type{T}, doc::Vector{AbstractString}, vocab::OrderedDict{AbstractString, Int}, window::Int, normalize::Bool, mode::Symbol)\n\nBasic low-level function that calculates the co-occurrence matrix of a document. Returns a sparse co-occurrence matrix sized n × n where n = length(vocab) with elements of type T. The document doc is represented by a vector of its terms (in order). The keywordswindowandnormalizeindicate the size of the sliding word window in which co-occurrences are counted and whether to normalize of not the counts by the distance between word positions. Themodekeyword can be either:defaultor:directionaland indicates whether the co-occurrence  matrix should be directional or not. This means that ifmodeis:directionalthen the co-occurrence matrix will be an × nmatrix wheren = length(vocab)andcoom[i,j]will be the number of timesvocab[i]co-occurs withvocab[j]in the documentdoc. Ifmodeis:defaultthen the co-occurrence matrix will be an × nmatrix wheren = length(vocab)andcoom[i,j]will be twice the number of timesvocab[i]co-occurs withvocab[j]in the documentdoc` (once for each direction, from i to j + from j to i).\n\nExample\n\njulia> using TextAnalysis, DataStructures\n       doc = StringDocument(\"This is a text about an apple. There are many texts about apples.\")\n       docv = TextAnalysis.tokenize(language(doc), text(doc))\n       vocab = OrderedDict(\"This\"=>1, \"is\"=>2, \"apple.\"=>3)\n       TextAnalysis.coo_matrix(Float16, docv, vocab, 5, true)\n\n3×3 SparseArrays.SparseMatrixCSC{Float16,Int64} with 4 stored entries:\n  [2, 1]  =  2.0\n  [1, 2]  =  2.0\n  [3, 2]  =  0.3999\n  [2, 3]  =  0.3999\n\njulia> using TextAnalysis, DataStructures\n       doc = StringDocument(\"This is a text about an apple. There are many texts about apples.\")\n       docv = TextAnalysis.tokenize(language(doc), text(doc))\n       vocab = OrderedDict(\"This\"=>1, \"is\"=>2, \"apple.\"=>3)\n       TextAnalysis.coo_matrix(Float16, docv, vocab, 5, true, :directional)\n\n3×3 SparseArrays.SparseMatrixCSC{Float16,Int64} with 4 stored entries:\n  [2, 1]  =  1.0\n  [1, 2]  =  1.0\n  [3, 2]  =  0.1999\n  [2, 3]  =  0.1999\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.coom-Tuple{CooMatrix}","page":"API References","title":"TextAnalysis.coom","text":"coom(c::CooMatrix)\n\nAccess the co-occurrence matrix field coom of a CooMatrix c.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.coom-Union{Tuple{Any}, Tuple{T}, Tuple{Any, Type{T}}} where T<:AbstractFloat","page":"API References","title":"TextAnalysis.coom","text":"coom(entity, eltype=DEFAULT_FLOAT_TYPE [;window=5, normalize=true])\n\nAccess the co-occurrence matrix of the CooMatrix associated with the entity. The CooMatrix{T} will first have to be created in order for the actual matrix to be accessed.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.cos_similarity-Tuple{AbstractMatrix}","page":"API References","title":"TextAnalysis.cos_similarity","text":"function cos_similarity(tfm::AbstractMatrix)\n\ncos_similarity calculates the cosine similarity from a term frequency matrix (typically the tf-idf matrix).\n\nExample\n\ncrps = Corpus( StringDocument.([\n    \"to be or not to be\",\n    \"to sing or not to sing\",\n    \"to talk or to silence\"]) )\nupdate_lexicon!(crps)\nd = dtm(crps)\ntfm = tf_idf(d)\ncs = cos_similarity(tfm)\nMatrix(cs)\n    # 3×3 Array{Float64,2}:\n    #  1.0        0.0329318  0.0\n    #  0.0329318  1.0        0.0\n    #  0.0        0.0        1.0\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.counter2-Tuple{Any, Integer, Integer}","page":"API References","title":"TextAnalysis.counter2","text":"counter2(\n    data,\n    min::Integer,\n    max::Integer\n) -> DataStructures.DefaultDict{SubString{String}, DataStructures.Accumulator{String, Int64}, DataStructures.Accumulator{SubString{String}, Int64}}\n\n\ncounter is used to make conditional distribution, which is used by score functions to  calculate conditional frequency distribution\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.dtm-Tuple{DocumentTermMatrix, Symbol}","page":"API References","title":"TextAnalysis.dtm","text":"dtm(crps::Corpus)\ndtm(d::DocumentTermMatrix)\ndtm(d::DocumentTermMatrix, density::Symbol)\n\nCreates a simple sparse matrix of DocumentTermMatrix object.\n\nExamples\n\njulia> crps = Corpus([StringDocument(\"To be or not to be\"),\n                      StringDocument(\"To become or not to become\")])\n\njulia> update_lexicon!(crps)\n\njulia> dtm(DocumentTermMatrix(crps))\n2×6 SparseArrays.SparseMatrixCSC{Int64,Int64} with 10 stored entries:\n  [1, 1]  =  1\n  [2, 1]  =  1\n  [1, 2]  =  2\n  [2, 3]  =  2\n  [1, 4]  =  1\n  [2, 4]  =  1\n  [1, 5]  =  1\n  [2, 5]  =  1\n  [1, 6]  =  1\n  [2, 6]  =  1\n\njulia> dtm(DocumentTermMatrix(crps), :dense)\n2×6 Array{Int64,2}:\n 1  2  0  1  1  1\n 1  0  2  1  1  1\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.dtv-Union{Tuple{T}, Tuple{AbstractDocument, Dict{T, Int64}}} where T","page":"API References","title":"TextAnalysis.dtv","text":"dtv(d::AbstractDocument, lex::Dict{String, Int})\n\nProduce a single row of a DocumentTermMatrix.\n\nIndividual documents do not have a lexicon associated with them, we have to pass in a lexicon as an additional argument.\n\nExamples\n\njulia> dtv(crps[1], lexicon(crps))\n1×6 Array{Int64,2}:\n 1  2  0  1  1  1\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.entropy-Tuple{TextAnalysis.Langmodel, DataStructures.DefaultDict, AbstractVector}","page":"API References","title":"TextAnalysis.entropy","text":"entropy(\n    m::TextAnalysis.Langmodel,\n    lm::DataStructures.DefaultDict,\n    text_ngram::AbstractVector\n) -> Float64\n\n\nCalculate cross-entropy of model for given evaluation text.\n\nInput text must be Vector of ngram of same lengths\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.everygram-Union{Tuple{Vector{T}}, Tuple{T}} where T<:AbstractString","page":"API References","title":"TextAnalysis.everygram","text":"everygram(seq::Vector{T}; min_len::Int=1, max_len::Int=-1)where { T <: AbstractString}\n\nReturn all possible ngrams generated from sequence of items, as an Array{String,1}\n\nExample\n\njulia> seq = [\"To\",\"be\",\"or\",\"not\"]\njulia> a = everygram(seq,min_len=1, max_len=-1)\n 10-element Array{Any,1}:\n  \"or\"          \n  \"not\"         \n  \"To\"          \n  \"be\"                  \n  \"or not\" \n  \"be or\"       \n  \"be or not\"   \n  \"To be or\"    \n  \"To be or not\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.extend!-Tuple{NaiveBayesClassifier, Any}","page":"API References","title":"TextAnalysis.extend!","text":"extend!(model::NaiveBayesClassifier, dictElement)\n\nAdd the dictElement to dictionary of the Classifier model.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.features-Tuple{AbstractDict, AbstractVector}","page":"API References","title":"TextAnalysis.features","text":"features(\n    fs::AbstractDict,\n    dict::AbstractVector\n) -> Vector{Int64}\n\n\nCompute an Array, mapping the value corresponding to elements of dict to the input AbstractDict.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.fit!-Tuple{NaiveBayesClassifier, AbstractVector{T} where T<:Integer, Any}","page":"API References","title":"TextAnalysis.fit!","text":"fit!(model::NaiveBayesClassifier, str, class)\nfit!(model::NaiveBayesClassifier, ::Features, class)\nfit!(model::NaiveBayesClassifier, ::StringDocument, class)\n\nFit the weights for the model on the input data.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.fmeasure_lcs","page":"API References","title":"TextAnalysis.fmeasure_lcs","text":"fmeasure_lcs(RLCS, PLCS, β)\n\nCompute the F-measure based on WLCS.\n\nArguments\n\nRLCS - Recall Factor\nPLCS - Precision Factor\nβ - Parameter\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.frequencies-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T","page":"API References","title":"TextAnalysis.frequencies","text":"frequencies(\n    xs::AbstractArray{T, 1}\n) -> Dict{_A, Int64} where _A\n\n\nCreate a dict that maps elements in input array to their frequencies.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.frequent_terms","page":"API References","title":"TextAnalysis.frequent_terms","text":"frequent_terms(crps, alpha=0.95)\n\nFind the frequent terms from Corpus, occurring more than alpha percentage of the documents.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"This is Document 1\"),\n                      StringDocument(\"This is Document 2\")])\nA Corpus with 2 documents:\n * 2 StringDocument's\n * 0 FileDocument's\n * 0 TokenDocument's\n * 0 NGramDocument's\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\njulia> frequent_terms(crps)\n3-element Array{String,1}:\n \"is\"\n \"This\"\n \"Document\"\n\nSee also: remove_frequent_terms!, sparse_terms\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.get_ngrams-Tuple{Vector{<:AbstractString}, Integer}","page":"API References","title":"TextAnalysis.get_ngrams","text":"get_ngrams(segment, max_order)\n\nExtracts all n-grams upto a given maximum order from an input segment. Returns the counter containing all n-grams upto max_order in segment with a count of how many times each n-gram occurred.\n\nArguments\n\nsegment: text segment from which n-grams will be extracted.\nmax_order: maximum length in tokens of the n-grams returned by this methods.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.hash_dtm-Tuple{Corpus, TextHashFunction}","page":"API References","title":"TextAnalysis.hash_dtm","text":"hash_dtm(crps::Corpus)\nhash_dtm(crps::Corpus, h::TextHashFunction)\n\nRepresents a Corpus as a Matrix with N entries.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.hash_dtv-Tuple{AbstractDocument, TextHashFunction}","page":"API References","title":"TextAnalysis.hash_dtv","text":"hash_dtv(d::AbstractDocument)\nhash_dtv(d::AbstractDocument, h::TextHashFunction)\n\nRepresents a document as a vector with N entries.\n\nExamples\n\njulia> crps = Corpus([StringDocument(\"To be or not to be\"),\n                      StringDocument(\"To become or not to become\")])\n\njulia> h = TextHashFunction(10)\nTextHashFunction(hash, 10)\n\njulia> hash_dtv(crps[1], h)\n1×10 Array{Int64,2}:\n 0  2  0  0  1  3  0  0  0  0\n\njulia> hash_dtv(crps[1])\n1×100 Array{Int64,2}:\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.index_hash-Tuple{AbstractString, TextHashFunction}","page":"API References","title":"TextAnalysis.index_hash","text":"index_hash(str, TextHashFunc)\n\nShows mapping of string to integer.\n\nParameters: \t-  str\t\t   = Max index used for hashing (default 100)  \t-  TextHashFunc    = TextHashFunction type object\n\njulia> h = TextHashFunction(10)\nTextHashFunction(hash, 10)\n\njulia> index_hash(\"a\", h)\n8\n\njulia> index_hash(\"b\", h)\n7\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.inverse_index-Tuple{Corpus}","page":"API References","title":"TextAnalysis.inverse_index","text":"inverse_index(crps::Corpus)\n\nShows the inverse index of a corpus.\n\nIf we are interested in a specific term, we often want to know which documents in a corpus contain that term. The inverse index tells us this and therefore provides a simplistic sort of search algorithm.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.language!-Tuple{AbstractDocument, Languages.Language}","page":"API References","title":"TextAnalysis.language!","text":"language!(doc, lang::Language)\n\nSet the language of doc to lang.\n\nExample\n\njulia> d = StringDocument(\"String Document 1\")\n\njulia> language!(d, Languages.Spanish())\n\njulia> d.metadata.language\nLanguages.Spanish()\n\nSee also: language, languages, languages!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.language-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.language","text":"language(doc)\n\nReturn the language metadata for doc.\n\nSee also: language!, languages, languages!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.languages!-Union{Tuple{T}, Tuple{Corpus, Vector{T}}} where T<:Languages.Language","page":"API References","title":"TextAnalysis.languages!","text":"languages!(crps, langs::Vector{Language})\nlanguages!(crps, lang::Language)\n\nUpdate languages of documents in a Corpus.\n\nIf the input is a Vector, then language of the ith document is set to the ith element in the vector, respectively. However, the number of documents must equal the length of vector.\n\nSee also: languages, language!, language\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.languages-Tuple{Corpus}","page":"API References","title":"TextAnalysis.languages","text":"languages(crps)\n\nReturn the languages for each document in crps.\n\nSee also: languages!, language, language!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.lda-Tuple{DocumentTermMatrix, Int64, Int64, Float64, Float64}","page":"API References","title":"TextAnalysis.lda","text":"ϕ, θ = lda(dtm::DocumentTermMatrix, ntopics::Int, iterations::Int, α::Float64, β::Float64; kwargs...)\n\nPerform Latent Dirichlet allocation.\n\nRequired Positional Arguments\n\nα Dirichlet dist. hyperparameter for topic distribution per document. α<1 yields a sparse topic mixture for each document. α>1 yields a more uniform topic mixture for each document.\nβ Dirichlet dist. hyperparameter for word distribution per topic. β<1 yields a sparse word mixture for each topic. β>1 yields a more uniform word mixture for each topic.\n\nOptional Keyword Arguments\n\nshowprogress::Bool. Show a progress bar during the Gibbs sampling. Default value: true.\n\nReturn Values\n\nϕ: ntopics × nwords Sparse matrix of probabilities s.t. sum(ϕ, 1) == 1\nθ: ntopics × ndocs Dense matrix of probabilities s.t. sum(θ, 1) == 1\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.lexical_frequency-Tuple{Corpus, AbstractString}","page":"API References","title":"TextAnalysis.lexical_frequency","text":"lexical_frequency(crps::Corpus, term::AbstractString)\n\nTells us how often a term occurs across all of the documents.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.lexicon-Tuple{Corpus}","page":"API References","title":"TextAnalysis.lexicon","text":"lexicon(crps::Corpus)\n\nShows the lexicon of the corpus.\n\nLexicon of a corpus consists of all the terms that occur in any document in the corpus.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"Name Foo\"),\n                          StringDocument(\"Name Bar\")])\nA Corpus with 2 documents:\n* 2 StringDocument's\n* 0 FileDocument's\n* 0 TokenDocument's\n* 0 NGramDocument's\n\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\n\njulia> lexicon(crps)\nDict{String,Int64} with 0 entries\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.lexicon_size-Tuple{Corpus}","page":"API References","title":"TextAnalysis.lexicon_size","text":"lexicon_size(crps::Corpus)\n\nTells the total number of terms in a lexicon.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.logscore-Tuple{TextAnalysis.Langmodel, DataStructures.DefaultDict, Any, Any}","page":"API References","title":"TextAnalysis.logscore","text":"logscore(\n    m::TextAnalysis.Langmodel,\n    temp_lm::DataStructures.DefaultDict,\n    word,\n    context\n) -> Float64\n\n\nEvaluate the log score of this word in this context.\n\nThe arguments are the same as for score and maskedscore\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.lookup-Union{Tuple{T}, Tuple{Vocabulary, AbstractVector{T}}} where T<:AbstractString","page":"API References","title":"TextAnalysis.lookup","text":"lookup(\n    voc::Vocabulary,\n    word::AbstractArray{T<:AbstractString, 1}\n) -> Vector\n\n\nlookup a sequence or words in the vocabulary\n\nReturn an Array of String\n\nSee Vocabulary\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.lsa-Tuple{DocumentTermMatrix}","page":"API References","title":"TextAnalysis.lsa","text":"lsa(dtm::DocumentTermMatrix)\nlsa(crps::Corpus)\n\nPerforms Latent Semantic Analysis or LSA on a corpus.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.maskedscore-Tuple{TextAnalysis.Langmodel, DataStructures.DefaultDict, Any, Any}","page":"API References","title":"TextAnalysis.maskedscore","text":"maskedscore(\n    m::TextAnalysis.Langmodel,\n    temp_lm::DataStructures.DefaultDict,\n    word,\n    context\n) -> Float64\n\n\nIt is used to evaluate score with masks out of vocabulary words\n\nThe arguments are the same as for score\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.ngramize-Union{Tuple{T}, Tuple{S}, Tuple{S, Vector{T}, Vararg{Integer}}} where {S<:Languages.Language, T<:AbstractString}","page":"API References","title":"TextAnalysis.ngramize","text":"ngramize(lang, tokens, n)\n\nCompute the ngrams of tokens of the order n.\n\nExample\n\njulia> ngramize(Languages.English(), [\"To\", \"be\", \"or\", \"not\", \"to\"], 3)\nDict{AbstractString,Int64} with 3 entries:\n  \"be or not\" => 1\n  \"or not to\" => 1\n  \"To be or\"  => 1\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.ngramizenew-Union{Tuple{T}, Tuple{Vector{T}, Vararg{Integer}}} where T<:AbstractString","page":"API References","title":"TextAnalysis.ngramizenew","text":"ngramizenew( words::Vector{T}, nlist::Integer...) where { T <: AbstractString}\n\nngramizenew is used to out putting ngrmas in set\n\nExample\n\njulia> seq=[\"To\",\"be\",\"or\",\"not\",\"To\",\"not\",\"To\",\"not\"]\njulia> ngramizenew(seq ,2)\n 7-element Array{Any,1}:\n  \"To be\" \n  \"be or\" \n  \"or not\"\n  \"not To\"\n  \"To not\"\n  \"not To\"\n  \"To not\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.ngrams-Tuple{NGramDocument, Integer}","page":"API References","title":"TextAnalysis.ngrams","text":"ngrams(ngd::NGramDocument, n::Integer)\nngrams(d::AbstractDocument, n::Integer)\nngrams(d::NGramDocument)\nngrams(d::AbstractDocument)\n\nAccess the document text as n-gram counts.\n\nExample\n\njulia> sd = StringDocument(\"To be or not to be...\")\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: To be or not to be...\n\njulia> ngrams(sd)\n Dict{String,Int64} with 7 entries:\n  \"or\"   => 1\n  \"not\"  => 1\n  \"to\"   => 1\n  \"To\"   => 1\n  \"be\"   => 1\n  \"be..\" => 1\n  \".\"    => 1\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.onegramize-Union{Tuple{T}, Tuple{S}, Tuple{S, Vector{T}}} where {S<:Languages.Language, T<:AbstractString}","page":"API References","title":"TextAnalysis.onegramize","text":"onegramize(lang, tokens)\n\nCreate the unigrams dict for input tokens.\n\nExample\n\njulia> onegramize(Languages.English(), [\"To\", \"be\", \"or\", \"not\", \"to\", \"be\"])\nDict{String,Int64} with 5 entries:\n  \"or\"  => 1\n  \"not\" => 1\n  \"to\"  => 1\n  \"To\"  => 1\n  \"be\"  => 2\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.padding_ngram-Union{Tuple{Vector{T}}, Tuple{T}, Tuple{Vector{T}, Any}} where T<:AbstractString","page":"API References","title":"TextAnalysis.padding_ngram","text":"padding_ngram(word::Vector{T}, n=1; pad_left=false, pad_right=false, left_pad_symbol=\"<s>\", right_pad_symbol =\"</s>\") where { T <: AbstractString}\n\npadding _ngram is used to pad both left and right of sentence and out putting ngrmas of order n\n\nIt also pad the original input Array of string \n\nExample\n\njulia> example = [\"1\",\"2\",\"3\",\"4\",\"5\"]\n\njulia> padding_ngram(example,2,pad_left=true,pad_right=true)\n 6-element Array{Any,1}:\n  \"<s> 1\" \n  \"1 2\"   \n  \"2 3\"   \n  \"3 4\"   \n  \"4 5\"   \n  \"5 </s>\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.perplexity-Tuple{TextAnalysis.Langmodel, DataStructures.DefaultDict, AbstractVector}","page":"API References","title":"TextAnalysis.perplexity","text":"perplexity(\n    m::TextAnalysis.Langmodel,\n    lm::DataStructures.DefaultDict,\n    text_ngram::AbstractVector\n) -> Float64\n\n\nCalculates the perplexity of the given text.\n\nThis is simply 2 ** cross-entropy(entropy) for the text, so the arguments are the same as entropy\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.predict-Tuple{NaiveBayesClassifier, AbstractVector{T} where T<:Integer}","page":"API References","title":"TextAnalysis.predict","text":"predict(::NaiveBayesClassifier, str)\npredict(::NaiveBayesClassifier, ::Features)\npredict(::NaiveBayesClassifier, ::StringDocument)\n\nPredict probabilities for each class on the input Features or String.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.prepare!-Tuple{Corpus, UInt32}","page":"API References","title":"TextAnalysis.prepare!","text":"prepare!(doc, flags)\nprepare!(crps, flags)\n\nPreprocess document or corpus based on the input flags.\n\nList of Flags\n\nstrip_patterns\nstripcorruptutf8\nstrip_case\nstem_words\ntagpartof_speech\nstrip_whitespace\nstrip_punctuation\nstrip_numbers\nstripnonletters\nstripindefinitearticles\nstripdefinitearticles\nstrip_articles\nstrip_prepositions\nstrip_pronouns\nstrip_stopwords\nstripsparseterms\nstripfrequentterms\nstriphtmltags\n\nExample\n\njulia> doc = StringDocument(\"This is a document of mine\")\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: This is a document of mine\njulia> prepare!(doc, strip_pronouns | strip_articles)\njulia> text(doc)\n\"This is   document of \"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.prob","page":"API References","title":"TextAnalysis.prob","text":"To get probability of word given that context\n\nIn other words, for given context calculate frequency distribution of word\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.prune!-Union{Tuple{T}, Tuple{DocumentTermMatrix{T}, Any}} where T","page":"API References","title":"TextAnalysis.prune!","text":"prune!(dtm::DocumentTermMatrix{T}, document_positions; compact::Bool=true, retain_terms::Union{Nothing,Vector{T}}=nothing) where {T}\n\nDelete documents specified by document_positions from a document term matrix. Optionally compact the matrix by removing unreferenced terms.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_case!-Tuple{FileDocument}","page":"API References","title":"TextAnalysis.remove_case!","text":"remove_case!(doc)\nremove_case!(crps)\n\nConvert the text of doc or crps to lowercase. Does not support FileDocument or crps containing FileDocument.\n\nExample\n\njulia> str = \"The quick brown fox jumps over the lazy dog\"\njulia> sd = StringDocument(str)\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: The quick brown fox jumps over the lazy dog\njulia> remove_case!(sd)\njulia> sd.text\n\"the quick brown fox jumps over the lazy dog\"\n\nSee also: remove_case\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_case-Tuple{T} where T<:AbstractString","page":"API References","title":"TextAnalysis.remove_case","text":"remove_case(str)\n\nConvert str to lowercase. See also: remove_case!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_corrupt_utf8!-Tuple{StringDocument}","page":"API References","title":"TextAnalysis.remove_corrupt_utf8!","text":"remove_corrupt_utf8!(doc)\nremove_corrupt_utf8!(crps)\n\nRemove corrupt UTF8 characters for doc or documents in crps. Does not support FileDocument or Corpus containing FileDocument. See also: remove_corrupt_utf8\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_corrupt_utf8-Tuple{AbstractString}","page":"API References","title":"TextAnalysis.remove_corrupt_utf8","text":"remove_corrupt_utf8(str)\n\nRemove corrupt UTF8 characters in str. See also: remove_corrupt_utf8!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_frequent_terms!","page":"API References","title":"TextAnalysis.remove_frequent_terms!","text":"remove_frequent_terms!(crps, alpha=0.95)\n\nRemove terms in crps, occurring more than alpha percent of documents.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"This is Document 1\"),\n                      StringDocument(\"This is Document 2\")])\nA Corpus with 2 documents:\n* 2 StringDocument's\n* 0 FileDocument's\n* 0 TokenDocument's\n* 0 NGramDocument's\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\njulia> remove_frequent_terms!(crps)\njulia> text(crps[1])\n\"     1\"\njulia> text(crps[2])\n\"     2\"\n\nSee also: remove_sparse_terms!, frequent_terms\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.remove_html_tags!-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.remove_html_tags!","text":"remove_html_tags!(doc::StringDocument)\nremove_html_tags!(crps)\n\nRemove html tags from the StringDocument or documents crps. Does not work for documents other than StringDocument.\n\nExample\n\njulia> html_doc = StringDocument(\n             \"\n               <html>\n                   <head><script language=\"javascript\">x = 20;</script></head>\n                   <body>\n                       <h1>Hello</h1><a href=\"world\">world</a>\n                   </body>\n               </html>\n             \"\n            )\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet:  <html> <head><s\njulia> remove_html_tags!(html_doc)\njulia> strip(text(html_doc))\n\"Hello world\"\n\nSee also: remove_html_tags\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_html_tags-Tuple{AbstractString}","page":"API References","title":"TextAnalysis.remove_html_tags","text":"remove_html_tags(str)\n\nRemove html tags from str, including the style and script tags. See also: remove_html_tags!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_patterns!-Tuple{FileDocument, Regex}","page":"API References","title":"TextAnalysis.remove_patterns!","text":"remove_patterns!(doc, rex::Regex)\nremove_patterns!(crps, rex::Regex)\n\nRemove patterns matched by rex in document or Corpus. Does not modify FileDocument or Corpus containing FileDocument. See also: remove_patterns\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_patterns-Tuple{AbstractString, Regex}","page":"API References","title":"TextAnalysis.remove_patterns","text":"remove_patterns(str, rex::Regex)\n\nRemove the part of str matched by rex. See also: remove_patterns!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_sparse_terms!","page":"API References","title":"TextAnalysis.remove_sparse_terms!","text":"remove_sparse_terms!(crps, alpha=0.05)\n\nRemove sparse terms in crps, occurring less than alpha percent of documents.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"This is Document 1\"),\n                      StringDocument(\"This is Document 2\")])\nA Corpus with 2 documents:\n * 2 StringDocument's\n * 0 FileDocument's\n * 0 TokenDocument's\n * 0 NGramDocument's\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\njulia> remove_sparse_terms!(crps, 0.5)\njulia> crps[1].text\n\"This is Document \"\njulia> crps[2].text\n\"This is Document \"\n\nSee also: remove_frequent_terms!, sparse_terms\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.remove_whitespace!-Tuple{StringDocument}","page":"API References","title":"TextAnalysis.remove_whitespace!","text":"remove_whitespace!(doc)\nremove_whitespace!(crps)\n\nSquash multiple whitespaces to a single space and remove all leading and trailing whitespaces in document or crps. Does no-op for FileDocument, TokenDocument or NGramDocument. See also: remove_whitespace\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_whitespace-Tuple{AbstractString}","page":"API References","title":"TextAnalysis.remove_whitespace","text":"remove_whitespace(str)\n\nSquash multiple whitespaces to a single one. And remove all leading and trailing whitespaces. See also: remove_whitespace!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.remove_words!-Union{Tuple{T}, Tuple{Union{AbstractDocument, Corpus}, Vector{T}}} where T<:AbstractString","page":"API References","title":"TextAnalysis.remove_words!","text":"remove_words!(doc, words::Vector{AbstractString})\nremove_words!(crps, words::Vector{AbstractString})\n\nRemove the occurrences of words from doc or crps.\n\nExample\n\njulia> str=\"The quick brown fox jumps over the lazy dog\"\njulia> sd=StringDocument(str);\njulia> remove_words = [\"fox\", \"over\"]\njulia> remove_words!(sd, remove_words)\njulia> sd.text\n\"the quick brown   jumps   the lazy dog\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.rouge_l_sentence","page":"API References","title":"TextAnalysis.rouge_l_sentence","text":"rouge_l_sentence(\n    references::Vector{<:AbstractString}, candidate::AbstractString, β=8;\n    weighted=false, weight_func=sqrt,\n    lang=Languages.English()\n)::Vector{Score}\n\nCalculate the ROUGE-L score between references and candidate at sentence level.\n\nReturns a vector of Score\n\nSee Rouge: A package for automatic evaluation of summaries\n\nNote: the weighted argument enables weighting of values when calculating the longest common subsequence. Initial implementation ROUGE-1.5.5.pl contains a power function. The function weight_func here has a power of 0.5 by default.\n\nSee also: rouge_n, rouge_l_summary\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.rouge_l_summary-Tuple{Vector{<:AbstractString}, AbstractString, Int64}","page":"API References","title":"TextAnalysis.rouge_l_summary","text":"rouge_l_summary(\n    references::Vector{<:AbstractString}, candidate::AbstractString, β::Int;\n    lang=Languages.English()\n)::Vector{Score}\n\nCalculate the ROUGE-L score between references and candidate at summary level.\n\nReturns a vector of Score\n\nSee Rouge: A package for automatic evaluation of summaries\n\nSee also: rouge_l_sentence(), rouge_n\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.rouge_n-Tuple{Vector{<:AbstractString}, AbstractString, Int64}","page":"API References","title":"TextAnalysis.rouge_n","text":"rouge_n(\n    references::Vector{<:AbstractString}, \n    candidate::AbstractString, \n    n::Int; \n    lang::Language\n)::Vector{Score}\n\nCompute n-gram recall between candidate and the references summaries.\n\nThe function takes the following arguments -\n\nreferences::Vector{T} where T<: AbstractString = The list of reference summaries.\ncandidate::AbstractString = Input candidate summary, to be scored against reference summaries.\nn::Integer = Order of NGrams\nlang::Language = Language of the text, useful while generating N-grams. Defaults value is Languages.English()\n\nReturns a vector of Score\n\nSee Rouge: A package for automatic evaluation of summaries\n\nSee also: rouge_l_sentence, rouge_l_summary\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.score","page":"API References","title":"TextAnalysis.score","text":"score(m::MLE, temp_lm::DefaultDict, word::AbstractString, context::AbstractString)\n\nscore is used to output probability of word given that context in MLE\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.score-2","page":"API References","title":"TextAnalysis.score","text":"score(m::InterpolatedLanguageModel, temp_lm::DefaultDict, word::AbstractString, context::AbstractString)\n\nscore is used to output probability of word given that context in InterpolatedLanguageModel\n\nApply Kneserney and WittenBell smoothing depending upon the sub-Type\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.score-Tuple{TextAnalysis.gammamodel, DataStructures.DefaultDict, Any, Any}","page":"API References","title":"TextAnalysis.score","text":"score(m::gammamodel, temp_lm::DefaultDict, word::AbstractString, context::AbstractString)\n\nscore is used to output probability of word given that context \n\nAdd-one smoothing to Lidstone or Laplace(gammamodel) models\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.sentence_tokenize-Union{Tuple{T}, Tuple{S}, Tuple{S, T}} where {S<:Languages.Language, T<:AbstractString}","page":"API References","title":"TextAnalysis.sentence_tokenize","text":"sentence_tokenize(language, str)\n\nSplit str into sentences.\n\nExample\n\njulia> sentence_tokenize(Languages.English(), \"Here are few words! I am Foo Bar.\")\n2-element Array{SubString{String},1}:\n \"Here are few words!\"\n \"I am Foo Bar.\"\n\nSee also: tokenize\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.sparse_terms","page":"API References","title":"TextAnalysis.sparse_terms","text":"sparse_terms(crps, alpha=0.05])\n\nFind the sparse terms from Corpus, occurring in less than alpha percentage of the documents.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"This is Document 1\"),\n                      StringDocument(\"This is Document 2\")])\nA Corpus with 2 documents:\n* 2 StringDocument's\n* 0 FileDocument's\n* 0 TokenDocument's\n* 0 NGramDocument's\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\njulia> sparse_terms(crps, 0.5)\n2-element Array{String,1}:\n \"1\"\n \"2\"\n\nSee also: remove_sparse_terms!, frequent_terms\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.standardize!-Union{Tuple{T}, Tuple{Corpus, Type{T}}} where T<:AbstractDocument","page":"API References","title":"TextAnalysis.standardize!","text":"standardize!(crps::Corpus, ::Type{T}) where T <: AbstractDocument\n\nStandardize the documents in a Corpus to a common type.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"Document 1\"),\n\t\t              TokenDocument(\"Document 2\"),\n\t\t              NGramDocument(\"Document 3\")])\nA Corpus with 3 documents:\n * 1 StringDocument's\n * 0 FileDocument's\n * 1 TokenDocument's\n * 1 NGramDocument's\n\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\n\n\njulia> standardize!(crps, NGramDocument)\n\n# After this step, you can check that the corpus only contains NGramDocument's:\n\njulia> crps\nA Corpus with 3 documents:\n * 0 StringDocument's\n * 0 FileDocument's\n * 0 TokenDocument's\n * 3 NGramDocument's\n\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.stem!-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.stem!","text":"stem!(doc)\nstem!(crps)\n\nStems the document or documents in crps with a suitable stemmer.\n\nStemming cannot be done for FileDocument and Corpus made of these type of documents.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.stem!-Tuple{Corpus}","page":"API References","title":"TextAnalysis.stem!","text":"stem!(crps::Corpus)\n\nStem an entire corpus. Assumes all documents in the corpus have the same language (picked from the first)\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.stemmer_for_document-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.stemmer_for_document","text":"stemmer_for_document(doc)\n\nSearch for an appropriate stemmer based on the language of the document.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.summarize-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.summarize","text":"summarize(doc [, ns])\n\nSummarizes the document and returns ns number of sentences. It takes 2 arguments:\n\nd : A document of type StringDocument, FileDocument or TokenDocument\nns : (Optional) Mention the number of sentences in the Summary, defaults to 5 sentences.\n\nBy default ns is set to the value 5.\n\nExample\n\njulia> s = StringDocument(\"Assume this Short Document as an example. Assume this as an example summarizer. This has too foo sentences.\")\n\njulia> summarize(s, ns=2)\n2-element Array{SubString{String},1}:\n \"Assume this Short Document as an example.\"\n \"This has too foo sentences.\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tag_scheme!-Tuple{Any, String, String}","page":"API References","title":"TextAnalysis.tag_scheme!","text":"tag_scheme!(tags, current_scheme::String, new_scheme::String)\n\nConvert tags from current_scheme to new_scheme.\n\nList of tagging schemes currently supported-\n\nBIO1 (BIO)\nBIO2\nBIOES\n\nExample\n\njulia> tags = [\"I-LOC\", \"O\", \"I-PER\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"I-PER\"]\n\njulia> tag_scheme!(tags, \"BIO1\", \"BIOES\")\n\njulia> tags\n8-element Array{String,1}:\n \"S-LOC\"\n \"O\"\n \"S-PER\"\n \"B-MISC\"\n \"E-MISC\"\n \"B-PER\"\n \"I-PER\"\n \"E-PER\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.text-Tuple{FileDocument}","page":"API References","title":"TextAnalysis.text","text":"text(fd::FileDocument)\ntext(sd::StringDocument)\ntext(ngd::NGramDocument)\n\nAccess the text of Document as a string.\n\nExample\n\njulia> sd = StringDocument(\"To be or not to be...\")\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: To be or not to be...\n\njulia> text(sd)\n\"To be or not to be...\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tf!-Union{Tuple{F}, Tuple{T}, Tuple{SparseArrays.SparseMatrixCSC{T}, SparseArrays.SparseMatrixCSC{F}}} where {T<:Real, F<:AbstractFloat}","page":"API References","title":"TextAnalysis.tf!","text":"tf!(dtm::SparseMatrixCSC{Real}, tf::SparseMatrixCSC{AbstractFloat})\n\nOverwrite tf with the term frequency of the dtm.\n\ntf should have the has same nonzeros as dtm.\n\nSee also: tf, tf_idf, tf_idf!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tf!-Union{Tuple{T2}, Tuple{T1}, Tuple{AbstractMatrix{T1}, AbstractMatrix{T2}}} where {T1<:Real, T2<:AbstractFloat}","page":"API References","title":"TextAnalysis.tf!","text":"tf!(dtm::AbstractMatrix{Real}, tf::AbstractMatrix{AbstractFloat})\n\nOverwrite tf with the term frequency of the dtm.\n\nWorks correctly if dtm and tf are same matrix.\n\nSee also: tf, tf_idf, tf_idf!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tf-Tuple{DocumentTermMatrix}","page":"API References","title":"TextAnalysis.tf","text":"tf(dtm::DocumentTermMatrix)\ntf(dtm::SparseMatrixCSC{Real})\ntf(dtm::Matrix{Real})\n\nCompute the term-frequency of the input.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"To be or not to be\"),\n              StringDocument(\"To become or not to become\")])\n\njulia> update_lexicon!(crps)\n\njulia> m = DocumentTermMatrix(crps)\n\njulia> tf(m)\n2×6 SparseArrays.SparseMatrixCSC{Float64,Int64} with 10 stored entries:\n  [1, 1]  =  0.166667\n  [2, 1]  =  0.166667\n  [1, 2]  =  0.333333\n  [2, 3]  =  0.333333\n  [1, 4]  =  0.166667\n  [2, 4]  =  0.166667\n  [1, 5]  =  0.166667\n  [2, 5]  =  0.166667\n  [1, 6]  =  0.166667\n  [2, 6]  =  0.166667\n\nSee also: tf!, tf_idf, tf_idf!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tf_idf!-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"API References","title":"TextAnalysis.tf_idf!","text":"tf_idf!(dtm)\n\nCompute tf-idf for dtm\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tf_idf!-Union{Tuple{F}, Tuple{T}, Tuple{SparseArrays.SparseMatrixCSC{T}, SparseArrays.SparseMatrixCSC{F}}} where {T<:Real, F<:AbstractFloat}","page":"API References","title":"TextAnalysis.tf_idf!","text":"tf_idf!(dtm::SparseMatrixCSC{Real}, tfidf::SparseMatrixCSC{AbstractFloat})\n\nOverwrite tfidf with the tf-idf (Term Frequency - Inverse Doc Frequency) of the dtm.\n\nThe arguments must have same number of nonzeros.\n\nSee also: tf, tf_idf, tf_idf!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tf_idf!-Union{Tuple{T2}, Tuple{T1}, Tuple{AbstractMatrix{T1}, AbstractMatrix{T2}}} where {T1<:Real, T2<:AbstractFloat}","page":"API References","title":"TextAnalysis.tf_idf!","text":"tf_idf!(dtm::AbstractMatrix{Real}, tf_idf::AbstractMatrix{AbstractFloat})\n\nOverwrite tf_idf with the tf-idf (Term Frequency - Inverse Doc Frequency) of the dtm.\n\ndtm and tf-idf must be matrices of same dimensions.\n\nSee also: tf, tf! , tf_idf\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tf_idf-Tuple{DocumentTermMatrix}","page":"API References","title":"TextAnalysis.tf_idf","text":"tf(dtm::DocumentTermMatrix)\ntf(dtm::SparseMatrixCSC{Real})\ntf(dtm::Matrix{Real})\n\nCompute tf-idf value (Term Frequency - Inverse Document Frequency) for the input.\n\nIn many cases, raw word counts are not appropriate for use because:\n\nSome documents are longer than other documents\nSome words are more frequent than other words\n\nA simple workaround this can be done by performing TF-IDF on a DocumentTermMatrix\n\nExample\n\njulia> crps = Corpus([StringDocument(\"To be or not to be\"),\n              StringDocument(\"To become or not to become\")])\n\njulia> update_lexicon!(crps)\n\njulia> m = DocumentTermMatrix(crps)\n\njulia> tf_idf(m)\n2×6 SparseArrays.SparseMatrixCSC{Float64,Int64} with 10 stored entries:\n  [1, 1]  =  0.0\n  [2, 1]  =  0.0\n  [1, 2]  =  0.231049\n  [2, 3]  =  0.231049\n  [1, 4]  =  0.0\n  [2, 4]  =  0.0\n  [1, 5]  =  0.0\n  [2, 5]  =  0.0\n  [1, 6]  =  0.0\n  [2, 6]  =  0.0\n\nSee also: tf!, tf_idf, tf_idf!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.timestamp!-Tuple{AbstractDocument, AbstractString}","page":"API References","title":"TextAnalysis.timestamp!","text":"timestamp!(doc, timestamp::AbstractString)\n\nSet the timestamp metadata of doc to timestamp.\n\nSee also: timestamp, timestamps, timestamps!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.timestamp-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.timestamp","text":"timestamp(doc)\n\nReturn the timestamp metadata for doc.\n\nSee also: timestamp!, timestamps, timestamps!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.timestamps!-Tuple{Corpus, Vector{String}}","page":"API References","title":"TextAnalysis.timestamps!","text":"timestamps!(crps, times::Vector{String})\ntimestamps!(crps, time::AbstractString)\n\nSet the timestamps of the documents in crps to the timestamps in times, respectively.\n\nSee also: timestamps, timestamp!, timestamp\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.timestamps-Tuple{Corpus}","page":"API References","title":"TextAnalysis.timestamps","text":"timestamps(crps)\n\nReturn the timestamps for each document in crps.\n\nSee also: timestamps!, timestamp, timestamp!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.title!-Tuple{AbstractDocument, AbstractString}","page":"API References","title":"TextAnalysis.title!","text":"title!(doc, str)\n\nSet the title of doc to str.\n\nSee also: title, titles, titles!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.title-Tuple{AbstractDocument}","page":"API References","title":"TextAnalysis.title","text":"title(doc)\n\nReturn the title metadata for doc.\n\nSee also: title!, titles, titles!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.titles!-Tuple{Corpus, Vector{String}}","page":"API References","title":"TextAnalysis.titles!","text":"titles!(crps, vec::Vector{String})\ntitles!(crps, str)\n\nUpdate titles of the documents in a Corpus.\n\nIf the input is a String, set the same title for all documents. If the input is a vector, set title of ith document to corresponding ith element in the vector vec. In the latter case, the number of documents must equal the length of vector.\n\nSee also: titles, title!, title\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.titles-Tuple{Corpus}","page":"API References","title":"TextAnalysis.titles","text":"titles(crps)\n\nReturn the titles for each document in crps.\n\nSee also: titles!, title, title!\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tokenize-Union{Tuple{T}, Tuple{S}, Tuple{S, T}} where {S<:Languages.Language, T<:AbstractString}","page":"API References","title":"TextAnalysis.tokenize","text":"tokenize(language, str)\n\nSplit str into words and other tokens such as punctuation.\n\nExample\n\njulia> tokenize(Languages.English(), \"Too foo words!\")\n4-element Array{String,1}:\n \"Too\"\n \"foo\"\n \"words\"\n \"!\"\n\nSee also: sentence_tokenize\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.tokens-Tuple{Union{FileDocument, StringDocument}}","page":"API References","title":"TextAnalysis.tokens","text":"tokens(d::TokenDocument)\ntokens(d::(Union{FileDocument, StringDocument}))\n\nAccess the document text as a token array.\n\nExample\n\njulia> sd = StringDocument(\"To be or not to be...\")\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: To be or not to be...\n\njulia> tokens(sd)\n7-element Array{String,1}:\n    \"To\"\n    \"be\"\n    \"or\"\n    \"not\"\n    \"to\"\n    \"be..\"\n    \".\"\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.update-Tuple{Vocabulary, Any}","page":"API References","title":"TextAnalysis.update","text":"update(vocab::Vocabulary, words) -> Dict{String, Int64}\n\n\nSee Vocabulary\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.weighted_lcs","page":"API References","title":"TextAnalysis.weighted_lcs","text":"weighted_lcs(X, Y, weight_score::Bool, returns_string::Bool, weigthing_function::Function)\n\nCompute the Weighted Longest Common Subsequence of X and Y.\n\n\n\n\n\n","category":"function"},{"location":"APIReference/#TextAnalysis.CooMatrix","page":"API References","title":"TextAnalysis.CooMatrix","text":"Basic Co-occurrence Matrix (COOM) type.\n\nFields\n\ncoom::SparseMatriCSC{T,Int} the actual COOM; elements represent\n\nco-occurrences of two terms within a given window\n\nterms::Vector{String} a list of terms that represent the lexicon of\n\nthe document or corpus\n\ncolumn_indices::OrderedDict{String, Int} a map between the terms and the\n\ncolumns of the co-occurrence matrix\n\n\n\n\n\n","category":"type"},{"location":"APIReference/#TextAnalysis.CooMatrix-Union{Tuple{T}, Tuple{Corpus, Vector{String}}} where T<:AbstractFloat","page":"API References","title":"TextAnalysis.CooMatrix","text":"CooMatrix{T}(crps::Corpus [,terms] [;window=5, normalize=true])\n\nAuxiliary constructor(s) of the CooMatrix type. The type T has to be a subtype of AbstractFloat. The constructor(s) requires a corpus crps and a terms structure representing the lexicon of the corpus. The latter can be a Vector{String}, an AbstractDict where the keys are the lexicon, or can be omitted, in which case the lexicon field of the corpus is used.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.Corpus-Union{Tuple{Vector{T}}, Tuple{T}} where T<:AbstractDocument","page":"API References","title":"TextAnalysis.Corpus","text":"Corpus(docs::Vector{T}) where {T <: AbstractDocument}\n\nCollections of documents are represented using the Corpus type.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"Document 1\"),\n\t\t              StringDocument(\"Document 2\")])\nA Corpus with 2 documents:\n * 2 StringDocument's\n * 0 FileDocument's\n * 0 TokenDocument's\n * 0 NGramDocument's\n\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.DocumentMetadata","page":"API References","title":"TextAnalysis.DocumentMetadata","text":"DocumentMetadata(\n    language::Language,\n    title::String,\n    author::String,\n    timestamp::String,\n    custom::Any\n)\n\nStores basic metadata about Document.\n\n...\n\nArguments\n\nlanguage: What language is the document in? Defaults to Languages.English(), a Language instance defined by the Languages package.\ntitle::String : What is the title of the document? Defaults to \"Untitled Document\".\nauthor::String : Who wrote the document? Defaults to \"Unknown Author\".\ntimestamp::String : When was the document written? Defaults to \"Unknown Time\".\ncustom : user specific data field. Defaults to nothing.\n\n...\n\n\n\n\n\n","category":"type"},{"location":"APIReference/#TextAnalysis.DocumentTermMatrix-Union{Tuple{T}, Tuple{Corpus, Vector{T}}} where T","page":"API References","title":"TextAnalysis.DocumentTermMatrix","text":"DocumentTermMatrix(crps::Corpus)\nDocumentTermMatrix(crps::Corpus, terms::Vector{String})\nDocumentTermMatrix(crps::Corpus, lex::AbstractDict)\nDocumentTermMatrix(dtm::SparseMatrixCSC{Int, Int},terms::Vector{String})\n\nRepresent documents as a matrix of word counts.\n\nAllow us to apply linear algebra operations and statistical techniques. Need to update lexicon before use.\n\nExamples\n\njulia> crps = Corpus([StringDocument(\"To be or not to be\"),\n                      StringDocument(\"To become or not to become\")])\n\njulia> update_lexicon!(crps)\n\njulia> m = DocumentTermMatrix(crps)\nA 2 X 6 DocumentTermMatrix\n\njulia> m.dtm\n2×6 SparseArrays.SparseMatrixCSC{Int64,Int64} with 10 stored entries:\n  [1, 1]  =  1\n  [2, 1]  =  1\n  [1, 2]  =  2\n  [2, 3]  =  2\n  [1, 4]  =  1\n  [2, 4]  =  1\n  [1, 5]  =  1\n  [2, 5]  =  1\n  [1, 6]  =  1\n  [2, 6]  =  1\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.FileDocument-Tuple{AbstractString}","page":"API References","title":"TextAnalysis.FileDocument","text":"FileDocument(pathname::AbstractString)\n\nRepresents a document using a plain text file on disk.\n\nExample\n\njulia> pathname = \"/usr/share/dict/words\"\n\"/usr/share/dict/words\"\n\njulia> fd = FileDocument(pathname)\nA FileDocument\n * Language: Languages.English()\n * Title: /usr/share/dict/words\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: A A's AMD AMD's AOL AOL's Aachen Aachen's Aaliyah\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.KneserNeyInterpolated-Union{Tuple{Vector{T}}, Tuple{T}, Tuple{Vector{T}, Any}, Tuple{Vector{T}, Any, Any}, Tuple{Vector{T}, Any, Any, Any}} where T<:AbstractString","page":"API References","title":"TextAnalysis.KneserNeyInterpolated","text":"KneserNeyInterpolated(word::Vector{T}, discount:: Float64,unk_cutoff=1, unk_label=\"<unk>\") where {T <: AbstractString}\n\nInitiate Type for providing KneserNey Interpolated language model.\n\nThe idea to abstract this comes from Chen & Goodman 1995.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.Laplace","page":"API References","title":"TextAnalysis.Laplace","text":"Laplace(word::Vector{T}, unk_cutoff=1, unk_label=\"<unk>\") where {T <: AbstractString}\n\nFunction to initiate Type(Laplace) for providing Laplace-smoothed scores.\n\nIn addition to initialization arguments from BaseNgramModel also requires a number by which to increase the counts, gamma = 1.\n\n\n\n\n\n","category":"type"},{"location":"APIReference/#TextAnalysis.Lidstone-Union{Tuple{Vector{T}}, Tuple{T}, Tuple{Vector{T}, Any}, Tuple{Vector{T}, Any, Any}, Tuple{Vector{T}, Any, Any, Any}} where T<:AbstractString","page":"API References","title":"TextAnalysis.Lidstone","text":"Lidstone(word::Vector{T}, gamma:: Float64, unk_cutoff=1, unk_label=\"<unk>\") where {T <: AbstractString}\n\nFunction to initiate Type(Lidstone) for providing Lidstone-smoothed scores.\n\nIn addition to initialization arguments from BaseNgramModel also requires  a number by which to increase the counts, gamma.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.MLE-Union{Tuple{Vector{T}}, Tuple{T}, Tuple{Vector{T}, Any}, Tuple{Vector{T}, Any, Any}} where T<:AbstractString","page":"API References","title":"TextAnalysis.MLE","text":"MLE(word::Vector{T}, unk_cutoff=1, unk_label=\"<unk>\") where {T <: AbstractString}\n\nInitiate Type for providing MLE ngram model scores.\n\nImplementation of Base Ngram Model.\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.NGramDocument-Tuple{AbstractString, TextAnalysis.DocumentMetadata, Vararg{Integer}}","page":"API References","title":"TextAnalysis.NGramDocument","text":"NGramDocument(txt::AbstractString, n::Integer=1)\nNGramDocument(txt::AbstractString, dm::DocumentMetadata, n::Integer=1)\nNGramDocument(ng::Dict{T, Int}, n::Integer=1) where T <: AbstractString\n\nRepresents a document as a bag of n-grams, which are UTF8 n-grams and map to counts.\n\nExample\n\njulia> my_ngrams = Dict{String, Int}(\"To\" => 1, \"be\" => 2,\n                                     \"or\" => 1, \"not\" => 1,\n                                     \"to\" => 1, \"be...\" => 1)\nDict{String,Int64} with 6 entries:\n  \"or\"    => 1\n  \"be...\" => 1\n  \"not\"   => 1\n  \"to\"    => 1\n  \"To\"    => 1\n  \"be\"    => 2\n\njulia> ngd = NGramDocument(my_ngrams)\nA NGramDocument{AbstractString}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: ***SAMPLE TEXT NOT AVAILABLE***\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.NaiveBayesClassifier-Tuple{Any, Any}","page":"API References","title":"TextAnalysis.NaiveBayesClassifier","text":"NaiveBayesClassifier([dict, ]classes)\n\nA Naive Bayes Classifier for classifying documents.\n\nIt takes two arguments:\n\nclasses: An array of possible classes that the concerned data could belong to.\ndict:(Optional Argument) An Array of possible tokens (words). This is automatically updated if a new token is detected in the Step 2) or 3)\n\nExample\n\njulia> using TextAnalysis: NaiveBayesClassifier, fit!, predict\n\njulia> m = NaiveBayesClassifier([:spam, :non_spam])\nNaiveBayesClassifier{Symbol}(String[], [:spam, :non_spam], Matrix{Int64}(undef, 0, 2))\n\njulia> fit!(m, \"this is spam\", :spam)\nNaiveBayesClassifier{Symbol}([\"this\", \"is\", \"spam\"], [:spam, :non_spam], [2 1; 2 1; 2 1])\n\njulia> fit!(m, \"this is not spam\", :non_spam)\nNaiveBayesClassifier{Symbol}([\"this\", \"is\", \"spam\", \"not\"], [:spam, :non_spam], [2 2; 2 2; 2 2; 1 2])\n\njulia> predict(m, \"is this a spam\")\nDict{Symbol, Float64} with 2 entries:\n  :spam     => 0.59883\n  :non_spam => 0.40117\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.Score","page":"API References","title":"TextAnalysis.Score","text":"struct Score\n\nprecision::Float32\nrecall::Float32\nfmeasure::Float32\n\n\n\n\n\n","category":"type"},{"location":"APIReference/#TextAnalysis.Score-Tuple{AbstractFloat, AbstractFloat, AbstractFloat}","page":"API References","title":"TextAnalysis.Score","text":"Score(\n    precision::AbstractFloat,\n    recall::AbstractFloat,\n    fmeasure::AbstractFloat\n) -> Score\n\n\nStores a result of evaluation\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.Score-Tuple{}","page":"API References","title":"TextAnalysis.Score","text":"Score(; precision, recall, fmeasure) -> Score\n\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.StringDocument-Tuple{AbstractString}","page":"API References","title":"TextAnalysis.StringDocument","text":"StringDocument(txt::AbstractString)\n\nRepresents a document using a UTF8 String stored in RAM.\n\nExample\n\njulia> str = \"To be or not to be...\"\n\"To be or not to be...\"\n\njulia> sd = StringDocument(str)\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: To be or not to be...\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.TextHashFunction-Tuple{Int64}","page":"API References","title":"TextAnalysis.TextHashFunction","text":"TextHashFunction(cardinality)\nTextHashFunction(hash_function, cardinality)\n\nThe need to create a lexicon before we can construct a document term matrix is often prohibitive. We can often employ a trick that has come to be called the Hash Trick in which we replace terms with their hashed valued using a hash function that outputs integers from 1 to N.\n\nParameters: \t-  cardinality\t    = Max index used for hashing (default 100)  \t-  hash_function    = function used for hashing process (default function present, see code-base)\n\njulia> h = TextHashFunction(10)\nTextHashFunction(hash, 10)\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.TokenDocument-Tuple{AbstractString, TextAnalysis.DocumentMetadata}","page":"API References","title":"TextAnalysis.TokenDocument","text":"TokenDocument(txt::AbstractString)\nTokenDocument(txt::AbstractString, dm::DocumentMetadata)\nTokenDocument(tkns::Vector{T}) where T <: AbstractString\n\nRepresents a document as a sequence of UTF8 tokens.\n\nExample\n\njulia> my_tokens = String[\"To\", \"be\", \"or\", \"not\", \"to\", \"be...\"]\n6-element Array{String,1}:\n    \"To\"\n    \"be\"\n    \"or\"\n    \"not\"\n    \"to\"\n    \"be...\"\n\njulia> td = TokenDocument(my_tokens)\nA TokenDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: ***SAMPLE TEXT NOT AVAILABLE***\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.Vocabulary","page":"API References","title":"TextAnalysis.Vocabulary","text":"Vocabulary(word,unk_cutoff =1 ,unk_label = \"<unk>\")\n\nStores language model vocabulary. Satisfies two common language modeling requirements for a vocabulary:\n\nWhen checking membership and calculating its size, filters items\n\nby comparing their counts to a cutoff value. Adds a special \"unknown\" token which unseen words are mapped to.\n\nExample\n\njulia> words = [\"a\", \"c\", \"-\", \"d\", \"c\", \"a\", \"b\", \"r\", \"a\", \"c\", \"d\"]\njulia> vocabulary = Vocabulary(words, 2) \n  Vocabulary(Dict(\"<unk>\"=>1,\"c\"=>3,\"a\"=>3,\"d\"=>2), 2, \"<unk>\") \n\njulia> vocabulary.vocab\n  Dict{String,Int64} with 4 entries:\n   \"<unk>\" => 1\n   \"c\"     => 3\n   \"a\"     => 3\n   \"d\"     => 2\n\nTokens with counts greater than or equal to the cutoff value will\nbe considered part of the vocabulary.\njulia> vocabulary.vocab[\"c\"]\n 3\n\njulia> \"c\" in keys(vocabulary.vocab)\n true\n\njulia> vocabulary.vocab[\"d\"]\n 2\n\njulia> \"d\" in keys(vocabulary.vocab)\n true\n\nTokens with frequency counts less than the cutoff value will be considered not\npart of the vocabulary even though their entries in the count dictionary are\npreserved.\njulia> \"b\" in keys(vocabulary.vocab)\n false\n\njulia> \"<unk>\" in keys(vocabulary.vocab)\n true\n\nWe can look up words in a vocabulary using its `lookup` method.\n\"Unseen\" words (with counts less than cutoff) are looked up as the unknown label.\nIf given one word (a string) as an input, this method will return a string.\njulia> lookup(\"a\")\n 'a'\n\njulia> word = [\"a\", \"-\", \"d\", \"c\", \"a\"]\n\njulia> lookup(vocabulary ,word)\n 5-element Array{Any,1}:\n  \"a\"    \n  \"<unk>\"\n  \"d\"    \n  \"c\"    \n  \"a\"\n\nIf given a sequence, it will return an Array{Any,1} of the looked up words as shown above.\n   \nIt's possible to update the counts after the vocabulary has been created.\njulia> update(vocabulary,[\"b\",\"c\",\"c\"])\n 1\n\njulia> vocabulary.vocab[\"b\"]\n 1\n\n\n\n\n\n","category":"type"},{"location":"APIReference/#TextAnalysis.Vocabulary-Union{Tuple{Vector{T}}, Tuple{T}, Tuple{Vector{T}, Any}, Tuple{Vector{T}, Any, Any}} where T<:AbstractString","page":"API References","title":"TextAnalysis.Vocabulary","text":"Vocabulary(word::Array{T<:AbstractString, 1}) -> Vocabulary\nVocabulary(\n    word::Array{T<:AbstractString, 1},\n    unk_cutoff\n) -> Vocabulary\nVocabulary(\n    word::Array{T<:AbstractString, 1},\n    unk_cutoff,\n    unk_label\n) -> Vocabulary\n\n\n\n\n\n\n","category":"method"},{"location":"APIReference/#TextAnalysis.WittenBellInterpolated-Union{Tuple{Vector{T}}, Tuple{T}, Tuple{Vector{T}, Any}, Tuple{Vector{T}, Any, Any}} where T<:AbstractString","page":"API References","title":"TextAnalysis.WittenBellInterpolated","text":"WittenBellInterpolated(word::Vector{T}, unk_cutoff=1, unk_label=\"<unk>\") where { T <: AbstractString}\n\nInitiate Type for providing Interpolated version of Witten-Bell smoothing.\n\nThe idea to abstract this comes from Chen & Goodman 1995.\n\n\n\n\n\n","category":"method"},{"location":"evaluation_metrics/#Evaluation-Metrics","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"","category":"section"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"Natural Language Processing tasks require certain Evaluation Metrics. As of now TextAnalysis provides the following evaluation metrics.","category":"page"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"ROUGE-N\nROUGE-L\nBLEU (bilingual evaluation understudy)","category":"page"},{"location":"evaluation_metrics/#ROUGE-N,-ROUGE-L,-ROUGE-L-Summary","page":"Evaluation Metrics","title":"ROUGE-N, ROUGE-L, ROUGE-L-Summary","text":"","category":"section"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"This metric evaluation based on the overlap of N-grams between the system and reference summaries.","category":"page"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"argmax\naverage\nrouge_n\nrouge_l_sentence\nrouge_l_summary","category":"page"},{"location":"evaluation_metrics/#Base.argmax","page":"Evaluation Metrics","title":"Base.argmax","text":"argmax(scores::Vector{Score})::Score\n\nscores - vector of Score\n\nReturns maximum by precision fiels of each Score\n\n\n\n\n\n","category":"function"},{"location":"evaluation_metrics/#TextAnalysis.average","page":"Evaluation Metrics","title":"TextAnalysis.average","text":"average(scores::Vector{Score})::Score\n\nscores - vector of Score\n\nReturns average values of scores as a Score with precision/recall/fmeasure\n\n\n\n\n\n","category":"function"},{"location":"evaluation_metrics/#TextAnalysis.rouge_n","page":"Evaluation Metrics","title":"TextAnalysis.rouge_n","text":"rouge_n(\n    references::Vector{<:AbstractString}, \n    candidate::AbstractString, \n    n::Int; \n    lang::Language\n)::Vector{Score}\n\nCompute n-gram recall between candidate and the references summaries.\n\nThe function takes the following arguments -\n\nreferences::Vector{T} where T<: AbstractString = The list of reference summaries.\ncandidate::AbstractString = Input candidate summary, to be scored against reference summaries.\nn::Integer = Order of NGrams\nlang::Language = Language of the text, useful while generating N-grams. Defaults value is Languages.English()\n\nReturns a vector of Score\n\nSee Rouge: A package for automatic evaluation of summaries\n\nSee also: rouge_l_sentence, rouge_l_summary\n\n\n\n\n\n","category":"function"},{"location":"evaluation_metrics/#TextAnalysis.rouge_l_sentence","page":"Evaluation Metrics","title":"TextAnalysis.rouge_l_sentence","text":"rouge_l_sentence(\n    references::Vector{<:AbstractString}, candidate::AbstractString, β=8;\n    weighted=false, weight_func=sqrt,\n    lang=Languages.English()\n)::Vector{Score}\n\nCalculate the ROUGE-L score between references and candidate at sentence level.\n\nReturns a vector of Score\n\nSee Rouge: A package for automatic evaluation of summaries\n\nNote: the weighted argument enables weighting of values when calculating the longest common subsequence. Initial implementation ROUGE-1.5.5.pl contains a power function. The function weight_func here has a power of 0.5 by default.\n\nSee also: rouge_n, rouge_l_summary\n\n\n\n\n\n","category":"function"},{"location":"evaluation_metrics/#TextAnalysis.rouge_l_summary","page":"Evaluation Metrics","title":"TextAnalysis.rouge_l_summary","text":"rouge_l_summary(\n    references::Vector{<:AbstractString}, candidate::AbstractString, β::Int;\n    lang=Languages.English()\n)::Vector{Score}\n\nCalculate the ROUGE-L score between references and candidate at summary level.\n\nReturns a vector of Score\n\nSee Rouge: A package for automatic evaluation of summaries\n\nSee also: rouge_l_sentence(), rouge_n\n\n\n\n\n\n","category":"function"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"using TextAnalysis\n\ncandidate_summary =  \"Brazil, Russia, China and India are growing nations. They are all an important part of BRIC as well as regular part of G20 summits.\"\nreference_summaries = [\"Brazil, Russia, India and China are the next big political powers in the global economy. Together referred to as BRIC(S) along with South Korea.\", \"Brazil, Russia, India and China are together known as the  BRIC(S) and have been invited to the G20 summit.\"]\n\nresults = [\n    rouge_n(reference_summaries, candidate_summary, 2),\n    rouge_n(reference_summaries, candidate_summary, 1)\n] .|> argmax","category":"page"},{"location":"evaluation_metrics/#BLEU-(bilingual-evaluation-understudy)","page":"Evaluation Metrics","title":"BLEU (bilingual evaluation understudy)","text":"","category":"section"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"bleu_score","category":"page"},{"location":"evaluation_metrics/#TextAnalysis.bleu_score","page":"Evaluation Metrics","title":"TextAnalysis.bleu_score","text":"bleu_score(reference_corpus::Vector{Vector{Token}}, translation_corpus::Vector{Token}; max_order=4, smooth=false)\n\nComputes BLEU score of translated segments against one or more references. Returns the BLEU score, n-gram precisions, brevity penalty,  geometric mean of n-gram precisions, translationlength and  referencelength\n\nArguments\n\nreference_corpus: list of lists of references for each translation. Each reference should be tokenized into a list of tokens.\ntranslation_corpus: list of translations to score. Each translation should be tokenized into a list of tokens.\nmax_order: maximum n-gram order to use when computing BLEU score. \nsmooth=false: whether or not to apply. Lin et al. 2004 smoothing.\n\nExample:\n\none_doc_references = [\n    [\"apple\", \"is\", \"apple\"],\n    [\"apple\", \"is\", \"a\", \"fruit\"]\n]  \none_doc_translation = [\n    \"apple\", \"is\", \"appl\"\n]\nbleu_score([one_doc_references], [one_doc_translation], smooth=true)\n\n\n\n\n\n","category":"function"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"NLTK sample","category":"page"},{"location":"evaluation_metrics/","page":"Evaluation Metrics","title":"Evaluation Metrics","text":"    using TextAnalysis\n\n    reference1 = [\n        \"It\", \"is\", \"a\", \"guide\", \"to\", \"action\", \"that\",\n        \"ensures\", \"that\", \"the\", \"military\", \"will\", \"forever\",\n        \"heed\", \"Party\", \"commands\"\n    ]\n    reference2 = [\n        \"It\", \"is\", \"the\", \"guiding\", \"principle\", \"which\",\n        \"guarantees\", \"the\", \"military\", \"forces\", \"always\",\n        \"being\", \"under\", \"the\", \"command\", \"of\", \"the\",\n        \"Party\"\n    ]\n    reference3 = [\n        \"It\", \"is\", \"the\", \"practical\", \"guide\", \"for\", \"the\",\n        \"army\", \"always\", \"to\", \"heed\", \"the\", \"directions\",\n        \"of\", \"the\", \"party\"\n    ]\n\n    hypothesis1 = [\n        \"It\", \"is\", \"a\", \"guide\", \"to\", \"action\", \"which\",\n        \"ensures\", \"that\", \"the\", \"military\", \"always\",\n        \"obeys\", \"the\", \"commands\", \"of\", \"the\", \"party\"\n    ]\n\n    score = bleu_score([[reference1, reference2, reference3]], [hypothesis1])","category":"page"},{"location":"features/#Creating-a-Document-Term-Matrix","page":"Features","title":"Creating a Document Term Matrix","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"Often we want to represent documents as a matrix of word counts so that we can apply linear algebra operations and statistical techniques. Before we do this, we need to update the lexicon:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"using TextAnalysis\ncrps = Corpus([StringDocument(\"To be or not to be\"),\n               StringDocument(\"To become or not to become\")])\nupdate_lexicon!(crps)\nm = DocumentTermMatrix(crps)","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"A DocumentTermMatrix object is a special type. If you would like to use a simple sparse matrix, call dtm() on this object:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> dtm(m)\n2×6 SparseArrays.SparseMatrixCSC{Int64,Int64} with 10 stored entries:\n  [1, 1]  =  1\n  [2, 1]  =  1\n  [1, 2]  =  2\n  [2, 3]  =  2\n  [1, 4]  =  1\n  [2, 4]  =  1\n  [1, 5]  =  1\n  [2, 5]  =  1\n  [1, 6]  =  1\n  [2, 6]  =  1","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"If you would like to use a dense matrix instead, you can pass this as an argument to the dtm function:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> dtm(m, :dense)\n2×6 Array{Int64,2}:\n 1  2  0  1  1  1\n 1  0  2  1  1  1","category":"page"},{"location":"features/#Creating-Individual-Rows-of-a-Document-Term-Matrix","page":"Features","title":"Creating Individual Rows of a Document Term Matrix","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"In many cases, we don't need the entire document term matrix at once: we can make do with just a single row. You can get this using the dtv function. Because individual's document do not have a lexicon associated with them, we have to pass in a lexicon as an additional argument:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> dtv(crps[1], lexicon(crps))\n1×6 Array{Int64,2}:\n 1  2  0  1  1  1","category":"page"},{"location":"features/#The-Hash-Trick","page":"Features","title":"The Hash Trick","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"The need to create a lexicon before we can construct a document term matrix is often prohibitive. We can often employ a trick that has come to be called the \"Hash Trick\" in which we replace terms with their hashed valued using a hash function that outputs integers from 1 to N. To construct such a hash function, you can use the TextHashFunction(N) constructor:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> h = TextHashFunction(10)\nTextHashFunction(hash, 10)","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"You can see how this function maps strings to numbers by calling the index_hash function:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> index_hash(\"a\", h)\n8\n\njulia> index_hash(\"b\", h)\n7","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"Using a text hash function, we can represent a document as a vector with N entries by calling the hash_dtv function:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> hash_dtv(crps[1], h)\n1×10 Array{Int64,2}:\n 0  2  0  0  1  3  0  0  0  0","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"This can be done for a corpus as a whole to construct a DTM without defining a lexicon in advance:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> hash_dtm(crps, h)\n2×10 Array{Int64,2}:\n 0  2  0  0  1  3  0  0  0  0\n 0  2  0  0  1  1  0  0  2  0","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"Every corpus has a hash function built-in, so this function can be called using just one argument:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> hash_dtm(crps)\n2×100 Array{Int64,2}:\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  2  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"Moreover, if you do not specify a hash function for just one row of the hash DTM, a default hash function will be constructed for you:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> hash_dtv(crps[1])\n1×100 Array{Int64,2}:\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0","category":"page"},{"location":"features/#TF-(Term-Frequency)","page":"Features","title":"TF (Term Frequency)","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"Often we need to find out the proportion of a document is contributed by each term. This can be done by finding the term frequency function","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"tf","category":"page"},{"location":"features/#TextAnalysis.tf","page":"Features","title":"TextAnalysis.tf","text":"tf(dtm::DocumentTermMatrix)\ntf(dtm::SparseMatrixCSC{Real})\ntf(dtm::Matrix{Real})\n\nCompute the term-frequency of the input.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"To be or not to be\"),\n              StringDocument(\"To become or not to become\")])\n\njulia> update_lexicon!(crps)\n\njulia> m = DocumentTermMatrix(crps)\n\njulia> tf(m)\n2×6 SparseArrays.SparseMatrixCSC{Float64,Int64} with 10 stored entries:\n  [1, 1]  =  0.166667\n  [2, 1]  =  0.166667\n  [1, 2]  =  0.333333\n  [2, 3]  =  0.333333\n  [1, 4]  =  0.166667\n  [2, 4]  =  0.166667\n  [1, 5]  =  0.166667\n  [2, 5]  =  0.166667\n  [1, 6]  =  0.166667\n  [2, 6]  =  0.166667\n\nSee also: tf!, tf_idf, tf_idf!\n\n\n\n\n\n","category":"function"},{"location":"features/","page":"Features","title":"Features","text":"The parameter, dtm can be of the types - DocumentTermMatrix , SparseMatrixCSC or Matrix","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"using TextAnalysis\ncrps = Corpus([StringDocument(\"To be or not to be\"),\n               StringDocument(\"To become or not to become\")])\nupdate_lexicon!(crps)\nm = DocumentTermMatrix(crps)\ntf(m)","category":"page"},{"location":"features/#TF-IDF-(Term-Frequency-Inverse-Document-Frequency)","page":"Features","title":"TF-IDF (Term Frequency - Inverse Document Frequency)","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"tf_idf","category":"page"},{"location":"features/#TextAnalysis.tf_idf","page":"Features","title":"TextAnalysis.tf_idf","text":"tf(dtm::DocumentTermMatrix)\ntf(dtm::SparseMatrixCSC{Real})\ntf(dtm::Matrix{Real})\n\nCompute tf-idf value (Term Frequency - Inverse Document Frequency) for the input.\n\nIn many cases, raw word counts are not appropriate for use because:\n\nSome documents are longer than other documents\nSome words are more frequent than other words\n\nA simple workaround this can be done by performing TF-IDF on a DocumentTermMatrix\n\nExample\n\njulia> crps = Corpus([StringDocument(\"To be or not to be\"),\n              StringDocument(\"To become or not to become\")])\n\njulia> update_lexicon!(crps)\n\njulia> m = DocumentTermMatrix(crps)\n\njulia> tf_idf(m)\n2×6 SparseArrays.SparseMatrixCSC{Float64,Int64} with 10 stored entries:\n  [1, 1]  =  0.0\n  [2, 1]  =  0.0\n  [1, 2]  =  0.231049\n  [2, 3]  =  0.231049\n  [1, 4]  =  0.0\n  [2, 4]  =  0.0\n  [1, 5]  =  0.0\n  [2, 5]  =  0.0\n  [1, 6]  =  0.0\n  [2, 6]  =  0.0\n\nSee also: tf!, tf_idf, tf_idf!\n\n\n\n\n\n","category":"function"},{"location":"features/","page":"Features","title":"Features","text":"In many cases, raw word counts are not appropriate for use because:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"(A) Some documents are longer than other documents\n(B) Some words are more frequent than other words","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"You can work around this by performing TF-IDF on a DocumentTermMatrix:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"using TextAnalysis\ncrps = Corpus([StringDocument(\"To be or not to be\"),\n               StringDocument(\"To become or not to become\")])\nupdate_lexicon!(crps)\nm = DocumentTermMatrix(crps)\ntf_idf(m)","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"As you can see, TF-IDF has the effect of inserting 0's into the columns of words that occur in all documents. This is a useful way to avoid having to remove those words during preprocessing.","category":"page"},{"location":"features/#Okapi-BM-25","page":"Features","title":"Okapi BM-25","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"From the document term matparamterix, Okapi BM25 document-word statistic can be created.","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"bm_25(dtm::AbstractMatrix; κ, β)\nbm_25(dtm::DocumentTermMatrixm, κ, β)","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"It can also be used via the following methods Overwrite the bm25 with calculated weights.","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"bm_25!(dtm, bm25, κ, β)","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"The inputs matrices can also be a Sparse Matrix. The parameters κ and β default to 2 and 0.75 respectively.","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"Here is an example usage -","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"using TextAnalysis\ncrps = Corpus([\n  StringDocument(\"a a a sample text text\"), \n  StringDocument(\"another example example text text\"), \n  StringDocument(\"\"), \n  StringDocument(\"another another text text text text\")\n])\nupdate_lexicon!(crps)\nm = DocumentTermMatrix(crps)\n\nbm_25(m)","category":"page"},{"location":"features/#Co-occurrence-matrix-(COOM)","page":"Features","title":"Co occurrence matrix (COOM)","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"The elements of the Co occurrence matrix indicate how many times two words co-occur in a (sliding) word window of a given size. The COOM can be calculated for objects of type Corpus, AbstractDocument (with the exception of NGramDocument).","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"CooMatrix(crps; window, normalize)\nCooMatrix(doc; window, normalize)","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"It takes following keyword arguments:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"window::Integer -length of the Window size, defaults to 5. The actual size of the sliding window is 2 * window + 1, with the keyword argument window specifying how many words to consider to the left and right of the center one\nnormalize::Bool -normalizes counts to distance between words, defaults to true","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"It returns the CooMatrix structure from which the matrix can be extracted using coom(::CooMatrix). The terms can also be extracted from this. Here is an example usage -","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"using TextAnalysis\ncrps = Corpus([StringDocument(\"this is a string document\")])\nC = CooMatrix(crps, window=1, normalize=false)\ncoom(C)\nC.terms","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"It can also be called to calculate the terms for a specific list of words / terms in the document. In other cases it calculates the the co occurrence elements for all the terms.","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"CooMatrix(crps, terms; window, normalize)\nCooMatrix(doc, terms; window, normalize)","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> C = CooMatrix(crps, [\"this\", \"is\", \"a\"], window=1, normalize=false)\nCooMatrix{Float64}(\n  [2, 1]  =  4.0\n  [1, 2]  =  4.0\n  [3, 2]  =  4.0\n  [2, 3]  =  4.0, [\"this\", \"is\", \"a\"], OrderedCollections.OrderedDict(\"this\"=>1,\"is\"=>2,\"a\"=>3))\n","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"The type can also be specified for CooMatrix with the weights of type T. T defaults to Float64.","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"CooMatrix{T}(crps; window, normalize) where T <: AbstractFloat\nCooMatrix{T}(doc; window, normalize) where T <: AbstractFloat\nCooMatrix{T}(crps, terms; window, normalize) where T <: AbstractFloat\nCooMatrix{T}(doc, terms; window, normalize) where T <: AbstractFloat","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"Remarks:","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"The sliding window used to count co-occurrences does not take into consideration sentence stops however, it does with documents i.e. does not span across documents\nThe co-occurrence matrices of the documents in a corpus are summed up when calculating the matrix for an entire corpus","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"note: Note\nThe Co occurrence matrix does not work for NGramDocument, or a Corpus containing an NGramDocument.","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"julia> C = CooMatrix(NGramDocument(\"A document\"), window=1, normalize=false) # fails, documents are NGramDocument\nERROR: The tokens of an NGramDocument cannot be reconstructed","category":"page"},{"location":"features/#Summarizer","page":"Features","title":"Summarizer","text":"","category":"section"},{"location":"features/","page":"Features","title":"Features","text":"TextAnalysis offers a simple text-rank based summarizer for its various document types.","category":"page"},{"location":"features/","page":"Features","title":"Features","text":"summarize","category":"page"},{"location":"features/#TextAnalysis.summarize","page":"Features","title":"TextAnalysis.summarize","text":"summarize(doc [, ns])\n\nSummarizes the document and returns ns number of sentences. It takes 2 arguments:\n\nd : A document of type StringDocument, FileDocument or TokenDocument\nns : (Optional) Mention the number of sentences in the Summary, defaults to 5 sentences.\n\nBy default ns is set to the value 5.\n\nExample\n\njulia> s = StringDocument(\"Assume this Short Document as an example. Assume this as an example summarizer. This has too foo sentences.\")\n\njulia> summarize(s, ns=2)\n2-element Array{SubString{String},1}:\n \"Assume this Short Document as an example.\"\n \"This has too foo sentences.\"\n\n\n\n\n\n","category":"function"},{"location":"documents/#Creating-Documents","page":"Documents","title":"Creating Documents","text":"","category":"section"},{"location":"documents/","page":"Documents","title":"Documents","text":"The basic unit of text analysis is a document. The TextAnalysis package allows one to work with documents stored in a variety of formats:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"FileDocument : A document represented using a plain text file on disk\nStringDocument : A document represented using a UTF8 String stored in RAM\nTokenDocument : A document represented as a sequence of UTF8 tokens\nNGramDocument : A document represented as a bag of n-grams, which are UTF8 n-grams that map to counts","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"note: Note\nThese formats represent a hierarchy: you can always move down the hierarchy, but can generally not move up the hierarchy. A FileDocument can easily become a StringDocument, but an NGramDocument cannot easily become a FileDocument.","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"Creating any of the four basic types of documents is very easy:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"StringDocument\nFileDocument\nTokenDocument\nNGramDocument","category":"page"},{"location":"documents/#TextAnalysis.StringDocument","page":"Documents","title":"TextAnalysis.StringDocument","text":"StringDocument(txt::AbstractString)\n\nRepresents a document using a UTF8 String stored in RAM.\n\nExample\n\njulia> str = \"To be or not to be...\"\n\"To be or not to be...\"\n\njulia> sd = StringDocument(str)\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: To be or not to be...\n\n\n\n\n\n","category":"type"},{"location":"documents/#TextAnalysis.FileDocument","page":"Documents","title":"TextAnalysis.FileDocument","text":"FileDocument(pathname::AbstractString)\n\nRepresents a document using a plain text file on disk.\n\nExample\n\njulia> pathname = \"/usr/share/dict/words\"\n\"/usr/share/dict/words\"\n\njulia> fd = FileDocument(pathname)\nA FileDocument\n * Language: Languages.English()\n * Title: /usr/share/dict/words\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: A A's AMD AMD's AOL AOL's Aachen Aachen's Aaliyah\n\n\n\n\n\n","category":"type"},{"location":"documents/#TextAnalysis.TokenDocument","page":"Documents","title":"TextAnalysis.TokenDocument","text":"TokenDocument(txt::AbstractString)\nTokenDocument(txt::AbstractString, dm::DocumentMetadata)\nTokenDocument(tkns::Vector{T}) where T <: AbstractString\n\nRepresents a document as a sequence of UTF8 tokens.\n\nExample\n\njulia> my_tokens = String[\"To\", \"be\", \"or\", \"not\", \"to\", \"be...\"]\n6-element Array{String,1}:\n    \"To\"\n    \"be\"\n    \"or\"\n    \"not\"\n    \"to\"\n    \"be...\"\n\njulia> td = TokenDocument(my_tokens)\nA TokenDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: ***SAMPLE TEXT NOT AVAILABLE***\n\n\n\n\n\n","category":"type"},{"location":"documents/#TextAnalysis.NGramDocument","page":"Documents","title":"TextAnalysis.NGramDocument","text":"NGramDocument(txt::AbstractString, n::Integer=1)\nNGramDocument(txt::AbstractString, dm::DocumentMetadata, n::Integer=1)\nNGramDocument(ng::Dict{T, Int}, n::Integer=1) where T <: AbstractString\n\nRepresents a document as a bag of n-grams, which are UTF8 n-grams and map to counts.\n\nExample\n\njulia> my_ngrams = Dict{String, Int}(\"To\" => 1, \"be\" => 2,\n                                     \"or\" => 1, \"not\" => 1,\n                                     \"to\" => 1, \"be...\" => 1)\nDict{String,Int64} with 6 entries:\n  \"or\"    => 1\n  \"be...\" => 1\n  \"not\"   => 1\n  \"to\"    => 1\n  \"To\"    => 1\n  \"be\"    => 2\n\njulia> ngd = NGramDocument(my_ngrams)\nA NGramDocument{AbstractString}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: ***SAMPLE TEXT NOT AVAILABLE***\n\n\n\n\n\n","category":"type"},{"location":"documents/","page":"Documents","title":"Documents","text":"An NGramDocument consisting of bigrams or any higher order representation N can be easily created by passing the parameter N to NGramDocument","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nNGramDocument(\"To be or not to be ...\", 2)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"For every type of document except a FileDocument, you can also construct a new document by simply passing in a string of text:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"To be or not to be...\")\ntd = TokenDocument(\"To be or not to be...\")\nngd = NGramDocument(\"To be or not to be...\")","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"The system will automatically perform tokenization or n-gramization in order to produce the required data. Unfortunately, FileDocument's cannot be constructed this way because filenames are themselves strings. It would cause chaos if filenames were treated as the text contents of a document.","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"That said, there is one way around this restriction: you can use the generic Document() constructor function, which will guess at the type of the inputs and construct the appropriate type of document object:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"julia> Document(\"To be or not to be...\")\nA StringDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: To be or not to be...\njulia> Document(\"/usr/share/dict/words\")\nA FileDocument\n * Language: Languages.English()\n * Title: /usr/share/dict/words\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: A A's AMD AMD's AOL AOL's Aachen Aachen's Aaliyah\n\njulia> Document(String[\"To\", \"be\", \"or\", \"not\", \"to\", \"be...\"])\nA TokenDocument{String}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: ***SAMPLE TEXT NOT AVAILABLE***\n\njulia> Document(Dict{String, Int}(\"a\" => 1, \"b\" => 3))\nA NGramDocument{AbstractString}\n * Language: Languages.English()\n * Title: Untitled Document\n * Author: Unknown Author\n * Timestamp: Unknown Time\n * Snippet: ***SAMPLE TEXT NOT AVAILABLE***","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"This constructor is very convenient for working in the REPL, but should be avoided in permanent code because, unlike the other constructors, the return type of the Document function cannot be known at compile-time.","category":"page"},{"location":"documents/#Basic-Functions-for-Working-with-Documents","page":"Documents","title":"Basic Functions for Working with Documents","text":"","category":"section"},{"location":"documents/","page":"Documents","title":"Documents","text":"Once you've created a document object, you can work with it in many ways. The most obvious thing is to access its text using the text() function:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"To be or not to be...\");\ntext(sd)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"note: Note\nThis function works without warnings on StringDocument's and FileDocument's. For TokenDocument's it is not possible to know if the text can be reconstructed perfectly, so calling text(TokenDocument(\"This is text\")) will produce a warning message before returning an approximate reconstruction of the text as it existed before tokenization. It is entirely impossible to reconstruct the text of an NGramDocument, so text(NGramDocument(\"This is text\")) raises an error.","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"Instead of working with the text itself, you can work with the tokens or n-grams of a document using the tokens() and ngrams() functions:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"To be or not to be...\");\ntokens(sd)\nngrams(sd)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"By default the ngrams() function produces unigrams. If you would like to produce bigrams or trigrams, you can specify that directly using a numeric argument to the ngrams() function:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"To be or not to be...\");\nngrams(sd, 2)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"The ngrams() function can also be called with multiple arguments:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"To be or not to be...\");\nngrams(sd, 2, 3)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"If you have a NGramDocument, you can determine whether an NGramDocument contains unigrams, bigrams or a higher-order representation using the ngram_complexity() function:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nngd = NGramDocument(\"To be or not to be ...\", 2);\nngram_complexity(ngd)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"This information is not available for other types of Document objects because it is possible to produce any level of complexity when constructing n-grams from raw text or tokens.","category":"page"},{"location":"documents/#Document-Metadata","page":"Documents","title":"Document Metadata","text":"","category":"section"},{"location":"documents/","page":"Documents","title":"Documents","text":"In addition to methods for manipulating the representation of the text of a document, every document object also stores basic metadata about itself, including the following pieces of information:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"language(): What language is the document in? Defaults to Languages.English(), a Language instance defined by the Languages package.\ntitle(): What is the title of the document? Defaults to \"Untitled Document\".\nauthor(): Who wrote the document? Defaults to \"Unknown Author\".\ntimestamp(): When was the document written? Defaults to \"Unknown Time\".","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"Try these functions out on a StringDocument to see how the defaults work in practice:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"This document has too foo words\")\nlanguage(sd)\ntitle(sd)\nauthor(sd)\ntimestamp(sd)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"If you need reset these fields, you can use the mutating versions of the same functions:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis, Languages\nsd = StringDocument(\"This document has too foo words\")\nlanguage!(sd, Languages.Spanish())\ntitle!(sd, \"El Cid\")\nauthor!(sd, \"Desconocido\")\ntimestamp!(sd, \"Desconocido\")","category":"page"},{"location":"documents/#Preprocessing-Documents","page":"Documents","title":"Preprocessing Documents","text":"","category":"section"},{"location":"documents/","page":"Documents","title":"Documents","text":"Having easy access to the text of a document and its metadata is very important, but most text analysis tasks require some amount of preprocessing.","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"At a minimum, your text source may contain corrupt characters. You can remove these using the remove_corrupt_utf8!() function:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"remove_corrupt_utf8!","category":"page"},{"location":"documents/#TextAnalysis.remove_corrupt_utf8!","page":"Documents","title":"TextAnalysis.remove_corrupt_utf8!","text":"remove_corrupt_utf8!(doc)\nremove_corrupt_utf8!(crps)\n\nRemove corrupt UTF8 characters for doc or documents in crps. Does not support FileDocument or Corpus containing FileDocument. See also: remove_corrupt_utf8\n\n\n\n\n\n","category":"function"},{"location":"documents/","page":"Documents","title":"Documents","text":"Alternatively, you may want to edit the text to remove items that are hard to process automatically. For example, our sample text sentence taken from Hamlet has three periods that we might like to discard. We can remove this kind of punctuation using the prepare!() function:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nstr = StringDocument(\"here are some punctuations !!!...\")\nprepare!(str, strip_punctuation)\ntext(str)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"To remove case distinctions, use remove_case!() function:\nAt times you'll want to remove specific words from a document like a person's","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"name. To do that, use the remove_words!() function:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"Lear is mad\")\nremove_case!(sd)\ntext(sd)\nremove_words!(sd, [\"lear\"])\ntext(sd)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"At other times, you'll want to remove whole classes of words. To make this easier, we can use several classes of basic words defined by the Languages.jl package:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"Articles : \"a\", \"an\", \"the\"\nIndefinite Articles : \"a\", \"an\"\nDefinite Articles : \"the\"\nPrepositions : \"across\", \"around\", \"before\", ...\nPronouns : \"I\", \"you\", \"he\", \"she\", ...\nStop Words : \"all\", \"almost\", \"alone\", ...","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"These special classes can all be removed using specially-named parameters:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"prepare!(sd, strip_articles)\nprepare!(sd, strip_indefinite_articles)\nprepare!(sd, strip_definite_articles)\nprepare!(sd, strip_prepositions)\nprepare!(sd, strip_pronouns)\nprepare!(sd, strip_stopwords)\nprepare!(sd, strip_numbers)\nprepare!(sd, strip_non_letters)\nprepare!(sd, strip_sparse_terms)\nprepare!(sd, strip_frequent_terms)\nprepare!(sd, strip_html_tags)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"These functions use words lists, so they are capable of working for many different languages without change, also these operations can be combined together for improved performance:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"prepare!(sd, strip_articles| strip_numbers| strip_html_tags)","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"In addition to removing words, it is also common to take words that are closely related like \"dog\" and \"dogs\" and stem them in order to produce a smaller set of words for analysis. We can do this using the stem!() function:","category":"page"},{"location":"documents/","page":"Documents","title":"Documents","text":"using TextAnalysis\nsd = StringDocument(\"They write, it writes\")\nstem!(sd)\ntext(sd)","category":"page"},{"location":"LM/#Statistical-Language-Model","page":"Statistical Language Model","title":"Statistical Language Model","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"TextAnalysis provide following different Language Models ","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"MLE - Base Ngram model.\nLidstone - Base Ngram model with Lidstone smoothing.\nLaplace - Base Ngram language model with Laplace smoothing.\nWittenBellInterpolated - Interpolated Version of witten-Bell algorithm.\nKneserNeyInterpolated - Interpolated  version of Kneser -Ney smoothing.","category":"page"},{"location":"LM/#APIs","page":"Statistical Language Model","title":"APIs","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"To use the API, we first Instantiate desired model and then load it with train set","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"MLE(word::Vector{T}, unk_cutoff=1, unk_label=\"<unk>\") where { T <: AbstractString}\n        \nLidstone(word::Vector{T}, gamma:: Float64, unk_cutoff=1, unk_label=\"<unk>\") where { T <: AbstractString}\n        \nLaplace(word::Vector{T}, unk_cutoff=1, unk_label=\"<unk>\") where { T <: AbstractString}\n        \nWittenBellInterpolated(word::Vector{T}, unk_cutoff=1, unk_label=\"<unk>\") where { T <: AbstractString}\n        \nKneserNeyInterpolated(word::Vector{T}, discount:: Float64=0.1, unk_cutoff=1, unk_label=\"<unk>\") where { T <: AbstractString}\n        \n(lm::<Languagemodel>)(text, min::Integer, max::Integer)","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"Arguments:","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"word : Array of  strings to store vocabulary.\nunk_cutoff: Tokens with counts greater than or equal to the cutoff value will be considered part of the vocabulary.\nunk_label: token for unknown labels \ngamma: smoothing argument gamma \ndiscount:  discounting factor for KneserNeyInterpolated\nfor more information see docstrings of vocabulary","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"julia> voc = [\"my\",\"name\",\"is\",\"salman\",\"khan\",\"and\",\"he\",\"is\",\"shahrukh\",\"Khan\"]\n\njulia> train = [\"khan\",\"is\",\"my\",\"good\", \"friend\",\"and\",\"He\",\"is\",\"my\",\"brother\"]\n# voc and train are used to train vocabulary and model respectively\n\njulia> model = MLE(voc)\nMLE(Vocabulary(Dict(\"khan\"=>1,\"name\"=>1,\"<unk>\"=>1,\"salman\"=>1,\"is\"=>2,\"Khan\"=>1,\"my\"=>1,\"he\"=>1,\"shahrukh\"=>1,\"and\"=>1…), 1, \"<unk>\", [\"my\", \"name\", \"is\", \"salman\", \"khan\", \"and\", \"he\", \"is\", \"shahrukh\", \"Khan\", \"<unk>\"]))\n\njulia> print(voc)\n11-element Array{String,1}:\n \"my\"\n \"name\"\n \"is\"\n \"salman\"\n \"khan\" \n \"and\" \n \"he\" \n \"is\"\n \"shahrukh\"\n \"Khan\"\n \"<unk>\"\n\n# you can see \"<unk>\" token is added to voc \njulia> fit = model(train,2,2) #considering only bigrams\n\njulia> unmaskedscore = score(model, fit, \"is\" ,\"<unk>\") #score output P(word | context) without replacing context word with \"<unk>\"\n0.3333333333333333\n\njulia> masked_score = maskedscore(model,fit,\"is\",\"alien\")\n0.3333333333333333\n#as expected maskedscore is equivalent to unmaskedscore with context replaced with \"<unk>\"\n","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"note: Note\nWhen you call MLE(voc) for the first time, It will update your vocabulary set as well. ","category":"page"},{"location":"LM/#Evaluation-Method","page":"Statistical Language Model","title":"Evaluation Method","text":"","category":"section"},{"location":"LM/#score","page":"Statistical Language Model","title":"score","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"used to evaluate the probability of word given context (P(word | context))","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"score","category":"page"},{"location":"LM/#TextAnalysis.score","page":"Statistical Language Model","title":"TextAnalysis.score","text":"score(m::gammamodel, temp_lm::DefaultDict, word::AbstractString, context::AbstractString)\n\nscore is used to output probability of word given that context \n\nAdd-one smoothing to Lidstone or Laplace(gammamodel) models\n\n\n\n\n\nscore(m::MLE, temp_lm::DefaultDict, word::AbstractString, context::AbstractString)\n\nscore is used to output probability of word given that context in MLE\n\n\n\n\n\nscore(m::InterpolatedLanguageModel, temp_lm::DefaultDict, word::AbstractString, context::AbstractString)\n\nscore is used to output probability of word given that context in InterpolatedLanguageModel\n\nApply Kneserney and WittenBell smoothing depending upon the sub-Type\n\n\n\n\n\n","category":"function"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"Arguments:","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"m : Instance of Langmodel struct.\ntemp_lm: output of function call of instance of Langmodel.\nword: string of word \ncontext: context of given word","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"In case of Lidstone and Laplace it apply smoothing and, \nIn Interpolated language model, provide Kneserney and WittenBell smoothing ","category":"page"},{"location":"LM/#maskedscore","page":"Statistical Language Model","title":"maskedscore","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"maskedscore","category":"page"},{"location":"LM/#TextAnalysis.maskedscore","page":"Statistical Language Model","title":"TextAnalysis.maskedscore","text":"maskedscore(\n    m::TextAnalysis.Langmodel,\n    temp_lm::DataStructures.DefaultDict,\n    word,\n    context\n) -> Float64\n\n\nIt is used to evaluate score with masks out of vocabulary words\n\nThe arguments are the same as for score\n\n\n\n\n\n","category":"function"},{"location":"LM/#logscore","page":"Statistical Language Model","title":"logscore","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"logscore","category":"page"},{"location":"LM/#TextAnalysis.logscore","page":"Statistical Language Model","title":"TextAnalysis.logscore","text":"logscore(\n    m::TextAnalysis.Langmodel,\n    temp_lm::DataStructures.DefaultDict,\n    word,\n    context\n) -> Float64\n\n\nEvaluate the log score of this word in this context.\n\nThe arguments are the same as for score and maskedscore\n\n\n\n\n\n","category":"function"},{"location":"LM/#entropy","page":"Statistical Language Model","title":"entropy","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"entropy","category":"page"},{"location":"LM/#TextAnalysis.entropy","page":"Statistical Language Model","title":"TextAnalysis.entropy","text":"entropy(\n    m::TextAnalysis.Langmodel,\n    lm::DataStructures.DefaultDict,\n    text_ngram::AbstractVector\n) -> Float64\n\n\nCalculate cross-entropy of model for given evaluation text.\n\nInput text must be Vector of ngram of same lengths\n\n\n\n\n\n","category":"function"},{"location":"LM/#perplexity","page":"Statistical Language Model","title":"perplexity","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"perplexity","category":"page"},{"location":"LM/#TextAnalysis.perplexity","page":"Statistical Language Model","title":"TextAnalysis.perplexity","text":"perplexity(\n    m::TextAnalysis.Langmodel,\n    lm::DataStructures.DefaultDict,\n    text_ngram::AbstractVector\n) -> Float64\n\n\nCalculates the perplexity of the given text.\n\nThis is simply 2 ** cross-entropy(entropy) for the text, so the arguments are the same as entropy\n\n\n\n\n\n","category":"function"},{"location":"LM/#Preprocessing","page":"Statistical Language Model","title":"Preprocessing","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"For Preprocessing following functions:","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"everygram\npadding_ngram","category":"page"},{"location":"LM/#TextAnalysis.everygram","page":"Statistical Language Model","title":"TextAnalysis.everygram","text":"everygram(seq::Vector{T}; min_len::Int=1, max_len::Int=-1)where { T <: AbstractString}\n\nReturn all possible ngrams generated from sequence of items, as an Array{String,1}\n\nExample\n\njulia> seq = [\"To\",\"be\",\"or\",\"not\"]\njulia> a = everygram(seq,min_len=1, max_len=-1)\n 10-element Array{Any,1}:\n  \"or\"          \n  \"not\"         \n  \"To\"          \n  \"be\"                  \n  \"or not\" \n  \"be or\"       \n  \"be or not\"   \n  \"To be or\"    \n  \"To be or not\"\n\n\n\n\n\n","category":"function"},{"location":"LM/#TextAnalysis.padding_ngram","page":"Statistical Language Model","title":"TextAnalysis.padding_ngram","text":"padding_ngram(word::Vector{T}, n=1; pad_left=false, pad_right=false, left_pad_symbol=\"<s>\", right_pad_symbol =\"</s>\") where { T <: AbstractString}\n\npadding _ngram is used to pad both left and right of sentence and out putting ngrmas of order n\n\nIt also pad the original input Array of string \n\nExample\n\njulia> example = [\"1\",\"2\",\"3\",\"4\",\"5\"]\n\njulia> padding_ngram(example,2,pad_left=true,pad_right=true)\n 6-element Array{Any,1}:\n  \"<s> 1\" \n  \"1 2\"   \n  \"2 3\"   \n  \"3 4\"   \n  \"4 5\"   \n  \"5 </s>\"\n\n\n\n\n\n","category":"function"},{"location":"LM/#Vocabulary","page":"Statistical Language Model","title":"Vocabulary","text":"","category":"section"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"Struct to store Language models vocabulary","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"checking membership and filters items by comparing their counts to a cutoff value","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"It also Adds a special \"unknown\" tokens which unseen words are mapped to","category":"page"},{"location":"LM/","page":"Statistical Language Model","title":"Statistical Language Model","text":"using TextAnalysis\nwords = [\"a\", \"c\", \"-\", \"d\", \"c\", \"a\", \"b\", \"r\", \"a\", \"c\", \"d\"]\nvocabulary = Vocabulary(words, 2) \n\n# lookup a sequence or words in the vocabulary\n\nword = [\"a\", \"-\", \"d\", \"c\", \"a\"]\n\nlookup(vocabulary ,word)","category":"page"},{"location":"classify/#Classifier","page":"Classifier","title":"Classifier","text":"","category":"section"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"Text Analysis currently offers a Naive Bayes Classifier for text classification.","category":"page"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"To load the Naive Bayes Classifier, use the following command -","category":"page"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"using TextAnalysis: NaiveBayesClassifier, fit!, predict","category":"page"},{"location":"classify/#Basic-Usage","page":"Classifier","title":"Basic Usage","text":"","category":"section"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"Its usage can be done in the following 3 steps.","category":"page"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"1- Create an instance of the Naive Bayes Classifier model -","category":"page"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"NaiveBayesClassifier","category":"page"},{"location":"classify/#TextAnalysis.NaiveBayesClassifier","page":"Classifier","title":"TextAnalysis.NaiveBayesClassifier","text":"NaiveBayesClassifier([dict, ]classes)\n\nA Naive Bayes Classifier for classifying documents.\n\nIt takes two arguments:\n\nclasses: An array of possible classes that the concerned data could belong to.\ndict:(Optional Argument) An Array of possible tokens (words). This is automatically updated if a new token is detected in the Step 2) or 3)\n\nExample\n\njulia> using TextAnalysis: NaiveBayesClassifier, fit!, predict\n\njulia> m = NaiveBayesClassifier([:spam, :non_spam])\nNaiveBayesClassifier{Symbol}(String[], [:spam, :non_spam], Matrix{Int64}(undef, 0, 2))\n\njulia> fit!(m, \"this is spam\", :spam)\nNaiveBayesClassifier{Symbol}([\"this\", \"is\", \"spam\"], [:spam, :non_spam], [2 1; 2 1; 2 1])\n\njulia> fit!(m, \"this is not spam\", :non_spam)\nNaiveBayesClassifier{Symbol}([\"this\", \"is\", \"spam\", \"not\"], [:spam, :non_spam], [2 2; 2 2; 2 2; 1 2])\n\njulia> predict(m, \"is this a spam\")\nDict{Symbol, Float64} with 2 entries:\n  :spam     => 0.59883\n  :non_spam => 0.40117\n\n\n\n\n\n","category":"type"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"2- Fitting the model weights on input -","category":"page"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"fit!","category":"page"},{"location":"classify/#TextAnalysis.fit!","page":"Classifier","title":"TextAnalysis.fit!","text":"fit!(model::NaiveBayesClassifier, str, class)\nfit!(model::NaiveBayesClassifier, ::Features, class)\nfit!(model::NaiveBayesClassifier, ::StringDocument, class)\n\nFit the weights for the model on the input data.\n\n\n\n\n\n","category":"function"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"3- Predicting for the input case -","category":"page"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"predict","category":"page"},{"location":"classify/#TextAnalysis.predict","page":"Classifier","title":"TextAnalysis.predict","text":"predict(::NaiveBayesClassifier, str)\npredict(::NaiveBayesClassifier, ::Features)\npredict(::NaiveBayesClassifier, ::StringDocument)\n\nPredict probabilities for each class on the input Features or String.\n\n\n\n\n\n","category":"function"},{"location":"classify/#Example","page":"Classifier","title":"Example","text":"","category":"section"},{"location":"classify/","page":"Classifier","title":"Classifier","text":"using TextAnalysis\nm = NaiveBayesClassifier([:legal, :financial])\nfit!(m, \"this is financial doc\", :financial)\nfit!(m, \"this is legal doc\", :legal)\npredict(m, \"this should be predicted as a legal document\")","category":"page"},{"location":"#Preface","page":"Home","title":"Preface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This manual is designed to get you started doing text analysis in Julia. It assumes that you already familiar with the basic methods of text analysis.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The TextAnalysis package can be installed using Julia's package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg.add(\"TextAnalysis\")","category":"page"},{"location":"#Loading","page":"Home","title":"Loading","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In all of the examples that follow, we'll assume that you have the TextAnalysis package fully loaded. This means that we think you've implicitly typed","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TextAnalysis","category":"page"},{"location":"","page":"Home","title":"Home","text":"before every snippet of code.","category":"page"},{"location":"#TextModels","page":"Home","title":"TextModels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The TextModels package enhances this library with the addition of practical neural network based models. Some of that code used to live in this package, but was moved to simplify installation and dependencies. ","category":"page"},{"location":"corpus/#Creating-a-Corpus","page":"Corpus","title":"Creating a Corpus","text":"","category":"section"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Working with isolated documents gets boring quickly. We typically want to work with a collection of documents. We represent collections of documents using the Corpus type:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Corpus","category":"page"},{"location":"corpus/#TextAnalysis.Corpus","page":"Corpus","title":"TextAnalysis.Corpus","text":"Corpus(docs::Vector{T}) where {T <: AbstractDocument}\n\nCollections of documents are represented using the Corpus type.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"Document 1\"),\n\t\t              StringDocument(\"Document 2\")])\nA Corpus with 2 documents:\n * 2 StringDocument's\n * 0 FileDocument's\n * 0 TokenDocument's\n * 0 NGramDocument's\n\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\n\n\n\n\n\n","category":"type"},{"location":"corpus/#Standardizing-a-Corpus","page":"Corpus","title":"Standardizing a Corpus","text":"","category":"section"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"A Corpus may contain many different types of documents. It is generally more convenient to standardize all of the documents in a corpus using a single type. This can be done using the standardize! function:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"standardize!","category":"page"},{"location":"corpus/#TextAnalysis.standardize!","page":"Corpus","title":"TextAnalysis.standardize!","text":"standardize!(crps::Corpus, ::Type{T}) where T <: AbstractDocument\n\nStandardize the documents in a Corpus to a common type.\n\nExample\n\njulia> crps = Corpus([StringDocument(\"Document 1\"),\n\t\t              TokenDocument(\"Document 2\"),\n\t\t              NGramDocument(\"Document 3\")])\nA Corpus with 3 documents:\n * 1 StringDocument's\n * 0 FileDocument's\n * 1 TokenDocument's\n * 1 NGramDocument's\n\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\n\n\njulia> standardize!(crps, NGramDocument)\n\n# After this step, you can check that the corpus only contains NGramDocument's:\n\njulia> crps\nA Corpus with 3 documents:\n * 0 StringDocument's\n * 0 FileDocument's\n * 0 TokenDocument's\n * 3 NGramDocument's\n\nCorpus's lexicon contains 0 tokens\nCorpus's index contains 0 tokens\n\n\n\n\n\n","category":"function"},{"location":"corpus/#Processing-a-Corpus","page":"Corpus","title":"Processing a Corpus","text":"","category":"section"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"We can apply the same sort of preprocessing steps that are defined for individual documents to an entire corpus at once:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"using TextAnalysis\ncrps = Corpus([StringDocument(\"Document ..!!\"),\n               StringDocument(\"Document ..!!\")])\nprepare!(crps, strip_punctuation)\ntext(crps[1])\ntext(crps[2])","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"These operations are run on each document in the corpus individually.","category":"page"},{"location":"corpus/#Corpus-Statistics","page":"Corpus","title":"Corpus Statistics","text":"","category":"section"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Often we wish to think broadly about properties of an entire corpus at once. In particular, we want to work with two constructs:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Lexicon: The lexicon of a corpus consists of all the terms that occur in any document in the corpus. The lexical frequency of a term tells us how often a term occurs across all of the documents. Often the most interesting words in a document are those words whose frequency within a document is higher than their frequency in the corpus as a whole.\nInverse Index: If we are interested in a specific term, we often want to know which documents in a corpus contain that term. The inverse index tells us this and therefore provides a simplistic sort of search algorithm.","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Because computations involving the lexicon can take a long time, a Corpus's default lexicon is blank:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> crps = Corpus([StringDocument(\"Name Foo\"),\n                          StringDocument(\"Name Bar\")])\njulia> lexicon(crps)\nDict{String,Int64} with 0 entries","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"In order to work with the lexicon, you have to update it and then access it:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> update_lexicon!(crps)\n\njulia> lexicon(crps)\nDict{String,Int64} with 3 entries:\n  \"Bar\"    => 1\n  \"Foo\"    => 1\n  \"Name\" => 2","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"But once this work is done, you can easier address lots of interesting questions about a corpus:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> lexical_frequency(crps, \"Name\")\n0.5\n\njulia> lexical_frequency(crps, \"Foo\")\n0.25","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Like the lexicon, the inverse index for a corpus is blank by default:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> inverse_index(crps)\nDict{String,Array{Int64,1}} with 0 entries","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Again, you need to update it before you can work with it:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> update_inverse_index!(crps)\n\njulia> inverse_index(crps)\nDict{String,Array{Int64,1}} with 3 entries:\n  \"Bar\"    => [2]\n  \"Foo\"    => [1]\n  \"Name\" => [1, 2]","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"But once you've updated the inverse index, you can easily search the entire corpus:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> crps[\"Name\"]\n\n2-element Array{Int64,1}:\n 1\n 2\n\njulia> crps[\"Foo\"]\n1-element Array{Int64,1}:\n 1\n\njulia> crps[\"Summer\"]\n0-element Array{Int64,1}","category":"page"},{"location":"corpus/#Converting-a-DataFrame-from-a-Corpus","page":"Corpus","title":"Converting a DataFrame from a Corpus","text":"","category":"section"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Sometimes we want to apply non-text specific data analysis operations to a corpus. The easiest way to do this is to convert a Corpus object into a DataFrame:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"convert(DataFrame, crps)","category":"page"},{"location":"corpus/#Corpus-Metadata","page":"Corpus","title":"Corpus Metadata","text":"","category":"section"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"You can also retrieve the metadata for every document in a Corpus at once:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"languages(): What language is the document in? Defaults to Languages.English(), a Language instance defined by the Languages package.\ntitles(): What is the title of the document? Defaults to \"Untitled Document\".\nauthors(): Who wrote the document? Defaults to \"Unknown Author\".\ntimestamps(): When was the document written? Defaults to \"Unknown Time\".","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> crps = Corpus([StringDocument(\"Name Foo\"),\n                                 StringDocument(\"Name Bar\")])\n\njulia> languages(crps)\n2-element Array{Languages.English,1}:\n Languages.English()\n Languages.English()\n\njulia> titles(crps)\n2-element Array{String,1}:\n \"Untitled Document\"\n \"Untitled Document\"\n\njulia> authors(crps)\n2-element Array{String,1}:\n \"Unknown Author\"\n \"Unknown Author\"\n\njulia> timestamps(crps)\n2-element Array{String,1}:\n \"Unknown Time\"\n \"Unknown Time\"","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"It is possible to change the metadata fields for each document in a Corpus. These functions use the same metadata value for every document:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> languages!(crps, Languages.German())\njulia> titles!(crps, \"\")\njulia> authors!(crps, \"Me\")\njulia> timestamps!(crps, \"Now\")","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"Additionally, you can specify the metadata fields for each document in a Corpus individually:","category":"page"},{"location":"corpus/","page":"Corpus","title":"Corpus","text":"julia> languages!(crps, [Languages.German(), Languages.English\njulia> titles!(crps, [\"\", \"Untitled\"])\njulia> authors!(crps, [\"Ich\", \"You\"])\njulia> timestamps!(crps, [\"Unbekannt\", \"2018\"])","category":"page"}]
}
