{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT\n",
    "An upgrade to BERT that advances the state-of-the-art performance on 12 NLP tasks\n",
    " \n",
    "The success of ALBERT demonstrates the importance of identifying the aspects of a model that give rise to powerful contextual representations. By focusing improvement efforts on these aspects of the model architecture, it is possible to greatly improve both the model efficiency and performance on a wide range of NLP tasks\n",
    "\n",
    "## Pretraining\n",
    "In this tutorial we are going to pre-train our albert model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia- Flux ALBERT \n",
    "It very easy and similar to any of the other Flux layer for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using TextAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ *ignore all the warning as TextAnalysis is checked out for developement*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using TextAnalysis.ALBERT # it is where our model reside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we are going to use DataDeps for handling download of pretrained model of ALBERT\n",
    "- For now we are directly laoding \n",
    "- other pretrained Weights can be found [here](https://drive.google.com/drive/u/1/folders/1HHTlS_jBYRE4cG0elITEH7fAkiNmrEgz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using WordTokenizers\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading spm tokenizer for albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordTokenizers.SentencePieceModel(Dict(\"▁shots\" => (-11.2373, 7281),\"▁ordered\" => (-9.84973, 1906),\"▁doubtful\" => (-12.7799, 22569),\"▁glancing\" => (-11.6676, 10426),\"▁disrespect\" => (-13.13, 26682),\"▁without\" => (-8.34227, 367),\"▁pol\" => (-10.7694, 4828),\"chem\" => (-12.3713, 17661),\"▁1947,\" => (-11.7544, 11199),\"▁kw\" => (-10.4402, 3511)…), 2)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = load(ALBERT_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`masksentence` - API to preprocess input text by appling mask for MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masksentence (generic function with 1 method)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function masksentence(words,\n",
    "                      spm;\n",
    "                      mask_token = \"[MASK]\",\n",
    "                      mask_ratio = 0.15,\n",
    "                      real_token_ratio = 0.1,\n",
    "                      random_token_ratio = 0.1)\n",
    "\n",
    "tokens = spm(words)\n",
    "masked_idx = randsubseq(1:length(tokens), mask_ratio)\n",
    "\n",
    "masked_tokens = copy(tokens)\n",
    "\n",
    "  for idx ∈ masked_idx\n",
    "    r = rand()\n",
    "    if r <= random_token_ratio\n",
    "      masked_tokens[idx] = rand(keys(spm.vocab_map))\n",
    "    elseif r > real_token_ratio + random_token_ratio\n",
    "      masked_tokens[idx] = mask_token\n",
    "    end\n",
    "  end\n",
    "\n",
    "  return masked_tokens, tokens, masked_idx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"▁i\", \"▁love\", \"▁julia\", \"▁language\"], [\"▁i\", \"▁love\", \"▁julia\", \"▁language\"], Int64[])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masksentence(\"i love julia language\",spm;\n",
    "                      mask_token = \"[MASK]\",\n",
    "                      mask_ratio = 0.15,\n",
    "                      real_token_ratio = 0.1,\n",
    "                      random_token_ratio = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mq\u001b[22m \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mq\u001b[22m!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "randsubseq([rng=GLOBAL_RNG,] A, p) -> Vector\n",
       "\\end{verbatim}\n",
       "Return a vector consisting of a random subsequence of the given array \\texttt{A}, where each element of \\texttt{A} is included (in order) with independent probability \\texttt{p}. (Complexity is linear in \\texttt{p*length(A)}, so this function is efficient even if \\texttt{p} is small and \\texttt{A} is large.) Technically, this process is known as \"Bernoulli sampling\" of \\texttt{A}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> rng = MersenneTwister(1234);\n",
       "\n",
       "julia> randsubseq(rng, collect(1:8), 0.3)\n",
       "2-element Array{Int64,1}:\n",
       " 7\n",
       " 8\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "randsubseq([rng=GLOBAL_RNG,] A, p) -> Vector\n",
       "```\n",
       "\n",
       "Return a vector consisting of a random subsequence of the given array `A`, where each element of `A` is included (in order) with independent probability `p`. (Complexity is linear in `p*length(A)`, so this function is efficient even if `p` is small and `A` is large.) Technically, this process is known as \"Bernoulli sampling\" of `A`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> rng = MersenneTwister(1234);\n",
       "\n",
       "julia> randsubseq(rng, collect(1:8), 0.3)\n",
       "2-element Array{Int64,1}:\n",
       " 7\n",
       " 8\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  randsubseq([rng=GLOBAL_RNG,] A, p) -> Vector\u001b[39m\n",
       "\n",
       "  Return a vector consisting of a random subsequence of the given array \u001b[36mA\u001b[39m,\n",
       "  where each element of \u001b[36mA\u001b[39m is included (in order) with independent probability\n",
       "  \u001b[36mp\u001b[39m. (Complexity is linear in \u001b[36mp*length(A)\u001b[39m, so this function is efficient even\n",
       "  if \u001b[36mp\u001b[39m is small and \u001b[36mA\u001b[39m is large.) Technically, this process is known as\n",
       "  \"Bernoulli sampling\" of \u001b[36mA\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> rng = MersenneTwister(1234);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> randsubseq(rng, collect(1:8), 0.3)\u001b[39m\n",
       "\u001b[36m  2-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   7\u001b[39m\n",
       "\u001b[36m   8\u001b[39m"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?randsubseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "using WordTokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "albert_pretrain_task(sentences,\n",
    "                       spm,\n",
    "                       sentences_pool = sentences;\n",
    "                       channel_size = 100,\n",
    "                       kwargs...\n",
    "                       )\n",
    "API for pretraining\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the implementation is inspired from \n",
    "https://nextjournal.com/chengchingwen/jsoc-2019-blog3end-of-phase-two-bert-model-in-julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albert_pretrain_task (generic function with 6 methods)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function albert_pretrain_task(sentences,\n",
    "                       spm,\n",
    "                       sentences_pool = sentences;\n",
    "                       channel_size = 100,\n",
    "                       kwargs...\n",
    "                       )\n",
    "  chn = Channel(channel_size)\n",
    "  task = @async albert_pretrain_task(chn, sentences, wordpiece, sentences_pool; kwargs...)\n",
    "  bind(chn, task)\n",
    "  chn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albert_pretrain_task (generic function with 6 methods)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function albert_pretrain_task(chn::Channel,\n",
    "                       sentences,\n",
    "                       spm,\n",
    "                       sentences_pool = sentences;\n",
    "                       start_token = \"[CLS]\",\n",
    "                       sep_token = \"[SEP]\",\n",
    "                       mask_token = \"[MASK]\",\n",
    "                       mask_ratio = 0.15,\n",
    "                       real_token_ratio = 0.1,\n",
    "                       random_token_ratio = 0.1,\n",
    "                       whole_word_mask = false,\n",
    "                       next_sentence_ratio = 0.5,\n",
    "                       next_sentence = true,\n",
    "                       return_real_sentence = false)\n",
    "\n",
    "  foreach(enumerate(sentences)) do (i, sentence)\n",
    "    sentenceA = masksentence(\n",
    "      sentence,\n",
    "      spm;\n",
    "      mask_token = mask_token,\n",
    "      mask_ratio = mask_ratio,\n",
    "      real_token_ratio = real_token_ratio,\n",
    "      random_token_ratio = random_token_ratio)\n",
    "    sentenceB = masksentence(\n",
    "        sentences[i+1],\n",
    "        spm;\n",
    "        mask_token = mask_token,\n",
    "        mask_ratio = mask_ratio,\n",
    "        real_token_ratio = real_token_ratio,\n",
    "        random_token_ratio = random_token_ratio)\n",
    "\n",
    "    if next_sentence\n",
    "      if rand() <= next_sentence_ratio && i != length(sentences)\n",
    "        isnext = true\n",
    "      else\n",
    "        temp = sentenceB\n",
    "        sentenceB = sentenceA\n",
    "        sentenceA = temp\n",
    "        isnext = false\n",
    "      end\n",
    "\n",
    "      masked_sentence = _wrap_sentence(sentenceA[1],\n",
    "                                       sentenceB[1];\n",
    "                                       start_token = start_token,\n",
    "                                       sep_token = sep_token)\n",
    "\n",
    "      sentence = _wrap_sentence(sentenceA[2],\n",
    "                                sentenceB[2];\n",
    "                                start_token = start_token,\n",
    "                                sep_token = sep_token)\n",
    "\n",
    "      mask_idx = _wrap_idx(sentenceA[3],\n",
    "                           sentenceB[3],\n",
    "                           length(sentenceA[1]))\n",
    "    else\n",
    "      masked_sentence = _wrap_sentence(sentenceA[1];\n",
    "                                       start_token = start_token,\n",
    "                                       sep_token = sep_token)\n",
    "\n",
    "      sentence = _wrap_sentence(sentenceA[2];\n",
    "                                start_token = start_token,\n",
    "                                sep_token = sep_token)\n",
    "\n",
    "      mask_idx = _wrap_idx(sentenceA[3])\n",
    "    end\n",
    "\n",
    "    masked_token = sentence[mask_idx]\n",
    "\n",
    "    if return_real_sentence\n",
    "      if next_sentence\n",
    "        put!(chn, (masked_sentence, mask_idx, masked_token, isnext, sentence))\n",
    "      else\n",
    "        put!(chn, (masked_sentence, mask_idx, masked_token, sentence))\n",
    "      end\n",
    "    else\n",
    "      if next_sentence\n",
    "        put!(chn, (masked_sentence, mask_idx, masked_token, isnext))\n",
    "      else\n",
    "        put!(chn, (masked_sentence, mask_idx, masked_token))\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_wrap_idx (generic function with 3 methods)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _wrap_sentence(sentence1, sentence2...; start_token = \"[CLS]\", sep_token = \"[SEP]\")\n",
    "  pushfirst!(sentence1, start_token)\n",
    "  push!(sentence1, sep_token)\n",
    "  map(s->push!(s, sep_token), sentence2)\n",
    "  vcat(sentence1, sentence2...)\n",
    "end\n",
    "\n",
    "_wrap_idx(sentence1_idx, pre_len = 1) = sentence1_idx .+= pre_len\n",
    "function _wrap_idx(sentence1_idx, sentence2_idx, len1)\n",
    "  _wrap_idx(sentence1_idx)\n",
    "  _wrap_idx(sentence2_idx, len1)\n",
    "  vcat(sentence1_idx, sentence2_idx)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albert_pretrain_task (generic function with 6 methods)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function albert_pretrain_task(outchn::Channel,\n",
    "                       datachn::Channel,\n",
    "                       spm;\n",
    "                       buffer_size = 100,\n",
    "                       kwargs...\n",
    "                       )\n",
    "  task = @async begin\n",
    "    buffer = Vector(undef, buffer_size)\n",
    "    while isopen(datachn)\n",
    "      i = 1\n",
    "      eod = false\n",
    "      while i <= buffer_size\n",
    "        try\n",
    "          sentence = take!(datachn)\n",
    "          if isempty(sentence)\n",
    "            continue\n",
    "          else\n",
    "            buffer[i] = sentence\n",
    "            i+=1\n",
    "          end\n",
    "        catch e\n",
    "          if isa(e, InvalidStateException) && e.state==:closed\n",
    "            eod = true\n",
    "            break\n",
    "          else\n",
    "            rethrow()\n",
    "          end\n",
    "        end\n",
    "      end\n",
    "\n",
    "      i -= 1\n",
    "\n",
    "      if eod || i == buffer_size\n",
    "        albert_pretrain_task(outchn, @view(buffer[1:(eod ? i - 1 : i)]), spm; kwargs...)\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  bind(outchn, task)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Guy Fawkes (; 13 April 1570�罱�� 31 January 1606), also known as Guido Fawkes while fighting for the Spanish, was a member of a group of provincial English Catholics who planned the failed Gunpowder Plot of 1605. He was born and educated in York, England; his father died when Fawkes was eight years old, after which his mother married a recusant Catholic.\\n\\nFawkes converted to Catholicism and left for mainland Europe, where he fought for Catholic Spain in the Eighty Years' War against Protestant Dutch reformers in the Low Countries. He travelled to Spain to seek support for a Catholic rebellion in England without success. He later met Thomas Wintour, with whom he returned to England, and Wintour introduced him to Robert Catesby, who planned to assassinate and restore a Catholic monarch to the throne. The plotters leased an undercroft beneath the House of Lords, and Fawkes was placed in charge of the gunpowder which they stockpiled there. The authorities were prompted by an anonymous letter to search Westminster Palace during the early hours of 5 November, and they found Fawkes guarding the explosives. He was questioned and tortured over the next few days, and he finally confessed.\\n\\nImmediately before his execution on 31 January, Fawkes fell from the scaffold where he was to be hanged and broke his neck, thus avoiding the agony of being hanged, drawn and quartered. He became synonymous with the Gunpowder Plot, the failure of which has been commemorated in Britain as Guy Fawkes Night since 5 November 1605, when his effigy is traditionally burned on a bonfire, commonly accompanied by fireworks.\\n\\nGuy Fawkes was born in 1570 in Stonegate, York. He was the second of four children born to Edward Fawkes, a proctor and an advocate of the consistory court at York, and his wife, Edith. Guy's parents were regular communicants of the Church of England, as were his paternal grandparents; his grandmother, born Ellen Harrington, was the daughter of a prominent merchant, who served as Lord Mayor of York in 1536. Guy's mother's family were recusant Catholics, and his cousin, Richard Cowling, became a Jesuit priest. \\\"Guy\\\" was an uncommon name in England, but may have been popular in York on account of a local notable, Sir Guy Fairfax of Steeton.\\n\\nThe date of Fawkes's birth is unknown, but he was baptised in the church of St Michael le Belfrey on 16 April. As the customary gap between birth and baptism was three days, he was probably born about 13 April. In 1568, Edith had given birth to a daughter named Anne, but the child died aged about seven weeks, in November that year. She bore two more children after Guy: Anne (b. 1572), and Elizabeth (b. 1575). Both were married, in 1599 and 1594 respectively.\\n\\nIn 1579, when Guy was eight years old, his father died. His mother remarried several years later, to the Catholic Dionis Baynbrigge (or Denis Bainbridge) of Scotton, Harrogate. Fawkes may have become a Catholic through the Baynbrigge family's recusant tendencies, and also the Catholic branches of the Pulleyn and Percy families of Scotton, but also from his time at St. Peter's School in York. A governor of the school had spent about 20�懢ears in prison for recusancy, and its headmaster, John Pulleyn, came from a family of noted Yorkshire recusants, the Pulleyns of Blubberhouses. In her 1915 work \\\"The Pulleynes of Yorkshire\\\", author Catharine Pullein suggested that Fawkes's Catholic education came from his Harrington relatives, who were known for harbouring priests, one of whom later accompanied Fawkes to Flanders in 1592��1593. Fawkes's fellow students included John Wright and his brother Christopher (both later involved with Fawkes in the Gunpowder Plot) and Oswald Tesimond, Edward Oldcorne and Robert Middleton, who became priests (the latter executed in 1601).\\n\""
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one document from wiki dump, just for illustration\n",
    "docs = \"\"\"\n",
    "Guy Fawkes (; 13 April 1570�罱�� 31 January 1606), also known as Guido Fawkes while fighting for the Spanish, was a member of a group of provincial English Catholics who planned the failed Gunpowder Plot of 1605. He was born and educated in York, England; his father died when Fawkes was eight years old, after which his mother married a recusant Catholic.\n",
    "\n",
    "Fawkes converted to Catholicism and left for mainland Europe, where he fought for Catholic Spain in the Eighty Years' War against Protestant Dutch reformers in the Low Countries. He travelled to Spain to seek support for a Catholic rebellion in England without success. He later met Thomas Wintour, with whom he returned to England, and Wintour introduced him to Robert Catesby, who planned to assassinate and restore a Catholic monarch to the throne. The plotters leased an undercroft beneath the House of Lords, and Fawkes was placed in charge of the gunpowder which they stockpiled there. The authorities were prompted by an anonymous letter to search Westminster Palace during the early hours of 5 November, and they found Fawkes guarding the explosives. He was questioned and tortured over the next few days, and he finally confessed.\n",
    "\n",
    "Immediately before his execution on 31 January, Fawkes fell from the scaffold where he was to be hanged and broke his neck, thus avoiding the agony of being hanged, drawn and quartered. He became synonymous with the Gunpowder Plot, the failure of which has been commemorated in Britain as Guy Fawkes Night since 5 November 1605, when his effigy is traditionally burned on a bonfire, commonly accompanied by fireworks.\n",
    "\n",
    "Guy Fawkes was born in 1570 in Stonegate, York. He was the second of four children born to Edward Fawkes, a proctor and an advocate of the consistory court at York, and his wife, Edith. Guy's parents were regular communicants of the Church of England, as were his paternal grandparents; his grandmother, born Ellen Harrington, was the daughter of a prominent merchant, who served as Lord Mayor of York in 1536. Guy's mother's family were recusant Catholics, and his cousin, Richard Cowling, became a Jesuit priest. \"Guy\" was an uncommon name in England, but may have been popular in York on account of a local notable, Sir Guy Fairfax of Steeton.\n",
    "\n",
    "The date of Fawkes's birth is unknown, but he was baptised in the church of St Michael le Belfrey on 16 April. As the customary gap between birth and baptism was three days, he was probably born about 13 April. In 1568, Edith had given birth to a daughter named Anne, but the child died aged about seven weeks, in November that year. She bore two more children after Guy: Anne (b. 1572), and Elizabeth (b. 1575). Both were married, in 1599 and 1594 respectively.\n",
    "\n",
    "In 1579, when Guy was eight years old, his father died. His mother remarried several years later, to the Catholic Dionis Baynbrigge (or Denis Bainbridge) of Scotton, Harrogate. Fawkes may have become a Catholic through the Baynbrigge family's recusant tendencies, and also the Catholic branches of the Pulleyn and Percy families of Scotton, but also from his time at St. Peter's School in York. A governor of the school had spent about 20�懢ears in prison for recusancy, and its headmaster, John Pulleyn, came from a family of noted Yorkshire recusants, the Pulleyns of Blubberhouses. In her 1915 work \"The Pulleynes of Yorkshire\", author Catharine Pullein suggested that Fawkes's Catholic education came from his Harrington relatives, who were known for harbouring priests, one of whom later accompanied Fawkes to Flanders in 1592��1593. Fawkes's fellow students included John Wright and his brother Christopher (both later involved with Fawkes in the Gunpowder Plot) and Oswald Tesimond, Edward Oldcorne and Robert Middleton, who became priests (the latter executed in 1601).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel{Any}(sz_max:3,sz_curr:3)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using WordTokenizers\n",
    "\n",
    "chn = Channel(3)\n",
    "\n",
    "sentences = split_sentences(docs)\n",
    "task = @async foreach(sentences) do sentence\n",
    "  if !isempty(sentence)\n",
    "    put!(chn, sentence)\n",
    "  end\n",
    "end\n",
    "bind(chn, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "albert_pretrain_task (generic function with 6 methods)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function albert_pretrain_task(datachn::Channel,\n",
    "                       spm;\n",
    "                       buffer_size = 100,\n",
    "                       channel_size = 100,\n",
    "                       kwargs...\n",
    "                       )\n",
    "  outchn = Channel(channel_size)\n",
    "  bert_pretrain_task(outchn, datachn, spm; buffer_size = buffer_size, kwargs...)\n",
    "  outchn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check our `albert_pretrain_task`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Transformers.Basic\n",
    "using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Array{T,1} where T,1}:\n",
       " [[\"[CLS]\", \"▁\", \"H\", \"[MASK]\", \"▁was\", \"[MASK]\", \"▁and\", \"▁tortured\", \"▁over\", \"▁the\"  …  \"▁found\", \"▁\", \"F\", \"aw\", \"kes\", \"▁guarding\", \"▁the\", \"[MASK]\", \".\", \"[SEP]\"]]\n",
       " [[4, 6, 14, 23, 24, 30, 41, 58, 61]]\n",
       " [[\"e\", \"▁questioned\", \",\", \"he\", \"▁authorities\", \"▁letter\", \"▁during\", \"kes\", \"▁explosives\"]]\n",
       " Bool[0]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = albert_pretrain_task(chn, spm)\n",
    "batch = get_batch(datas ,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it is working fine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Array{T,1} where T,1}:\n",
       " [[\"[CLS]\", \"▁\", \"H\", \"e\", \"▁was\", \"▁questioned\", \"▁and\", \"▁tortured\", \"▁over\", \"▁the\"  …  \"▁being\", \"▁hanged\", \",\", \"▁drawn\", \"▁and\", \"▁qu\", \"arte\", \"red\", \".\", \"[SEP]\"]]\n",
       " [[9, 19, 25, 31, 38, 46, 55, 58]]\n",
       " [[\"▁over\", \".\", \"ate\", \"▁31\", \"F\", \"▁he\", \",\", \"▁the\"]]\n",
       " Bool[1]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentence, mask_idx, masked_token, isnext = get_batch(datas, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using following libary as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.KeySet for a Dict{String,Tuple{Float64,Int64}} with 30000 entries. Keys:\n",
       "  \"▁shots\"\n",
       "  \"▁ordered\"\n",
       "  \"▁doubtful\"\n",
       "  \"▁glancing\"\n",
       "  \"▁disrespect\"\n",
       "  \"▁without\"\n",
       "  \"▁pol\"\n",
       "  \"chem\"\n",
       "  \"▁1947,\"\n",
       "  \"▁kw\"\n",
       "  \"▁calcutta\"\n",
       "  \"mh\"\n",
       "  \"▁rumors\"\n",
       "  \"▁maharaja\"\n",
       "  \"▁125\"\n",
       "  \"▁xanth\"\n",
       "  \"rha\"\n",
       "  \"▁pound\"\n",
       "  \"lunk\"\n",
       "  \"▁spaniards\"\n",
       "  \"▁ulcer\"\n",
       "  \"henry\"\n",
       "  \"228\"\n",
       "  \"izes\"\n",
       "  \"▁assist\"\n",
       "  ⋮"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TextAnalysis.ALBERT\n",
    "using Transformers.Basic\n",
    "vocab = keys(spm.vocab_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets define embedding layers\n",
    "The Embed is similar to nn.model in pytorch and is already implemented in Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompositeEmbedding(tok = Embed(300), pe = PositionEmbedding(300), seg = Embed(300))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = CompositeEmbedding(\n",
    "  tok = Embed(300, length(vocab)),\n",
    "  pe = PositionEmbedding(300, 512; trainable=false),\n",
    "  seg = Embed(300, 2)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Flux:onehotbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformerModel` is structure to holding embedding, transformers and classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel{TextAnalysis.ALBERT.albert_transformer}(\n",
       "  embed = CompositeEmbedding(tok = Embed(300), pe = PositionEmbedding(300), seg = Embed(300)),\n",
       "  transformers = albert(layers=3, head=12, head_size=25, pwffn_size=512, size=300),\n",
       "  classifier = \n",
       "    (\n",
       "      mlm => Dense(300, 300)\n",
       "      ns => Chain(Dense(300, 2), logsoftmax)\n",
       "    )\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert = ALBERT.albert_transformer(300,300,12,512,3,1,1) # defining albert_trainformer \n",
    "masklm = Flux.Dense(300,300) # masklm classifier\n",
    "nextsentence = Flux.Chain(Flux.Dense(300, 2), Flux.logsoftmax) # nextsentence classifiers\n",
    "\n",
    "albert_model = TransformerModel(emb, albert, (mlm=masklm, ns = nextsentence)) #struture to hold everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "`preprocess`- It will take care of proprocessing of text before moving it to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flux.Optimise.ADAM(0.0001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function preprocess(training_batch)\n",
    "    mask = getmask(training_batch[1])\n",
    "    tok = [(ids_from_tokens(spm,i)) for i in training_batch[1]]\n",
    "    tok = Flux.batchseq(tok,1)\n",
    "    tok = Flux.stack(tok,1)\n",
    "    segment = fill!(similar(tok), 1.0)\n",
    "    length(tok) #output embedding matrix\n",
    "     for (i, sentence) ∈ enumerate(training_batch[1])\n",
    "    j = findfirst(isequal(\"[SEP]\"), sentence)\n",
    "    if j !== nothing\n",
    "      @view(segment[j+1:end, i]) .= 2.0\n",
    "    end\n",
    "  end\n",
    "    \n",
    "    ind = vcat(\n",
    "    map(enumerate(batch[2])) do (i, x)\n",
    "     map(j->(j,i), x)\n",
    "    end...)\n",
    "\n",
    "  masklabel = onehotbatch(ids_from_tokens(spm , vcat(batch[3]...)), 1:length(spm.vocab_map))\n",
    "  nextlabel = onehotbatch(batch[4], (true, false))\n",
    "return (tok=tok, seg=segment), ind, masklabel, nextlabel, mask\n",
    "end\n",
    "\n",
    "function loss(data, ind, masklabel, nextlabel, mask = nothing)\n",
    "  e = albert_model.embed(data)\n",
    "  t = albert_model.transformers(e, mask)\n",
    "  nextloss = Basic.logcrossentropy(\n",
    "    nextlabel,\n",
    "    albert_model.classifier.ns(\n",
    "      t[:,1,:]\n",
    "    )\n",
    "  )\n",
    "  mkloss = masklmloss(albert_model.embed.embeddings.tok, # embedding table for compute similarity\n",
    "                      albert_model.classifier.mlm, # transform function on output embedding\n",
    "                      t, # output embeddings\n",
    "                      ind, # mask index\n",
    "                      masklabel #masked token\n",
    "                      )\n",
    "  return nextloss + mkloss\n",
    "end\n",
    "    \n",
    "ps = Flux.params(albert)\n",
    "opt = Flux.ADAM(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Channel{Any}(sz_max:100,sz_curr:0)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = albert_pretrain_task(chn, spm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets analysis the loss by running 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l = 72.28404f0\n",
      "l = 73.165596f0\n",
      "l = 56.124104f0\n",
      "l = 50.31461f0\n",
      "l = 51.023262f0\n",
      "l = 49.547054f0\n",
      "l = 43.89146f0\n",
      "l = 38.276382f0\n",
      "l = 48.87205f0\n",
      "l = 33.408596f0\n"
     ]
    }
   ],
   "source": [
    "for i ∈ 1:10 # run 10 step for illustration\n",
    "  batch = get_batch(datas, 2)\n",
    "  batch === nothing && break # out of data\n",
    "  data, ind, masklabel, nextlabel, mask = todevice(preprocess(batch))\n",
    "  l = loss(data, ind, masklabel, nextlabel, mask)\n",
    "  @show l\n",
    "  grad = Flux.gradient(()->loss(data, ind, masklabel, nextlabel, mask), ps)\n",
    "  Flux.update!(opt, ps, grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "As expected loss is converging for our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
