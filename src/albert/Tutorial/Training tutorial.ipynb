{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT Fine tuning Tutorial\n",
    "In this tutorial, we will be going through usage of SOTA transformers. We will be using ALBERT transformer model for this tutorial. You can check this link to understand more about [ALBERT](https://arxiv.org/abs/1909.11942)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code can be founded in PR [#203](https://github.com/JuliaText/TextAnalysis.jl/pull/203)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the following library for our tutorial\n",
    "- TextAnlaysis.ALBERT\n",
    "- WordTokenizer \n",
    "- Transformers and Flux \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling TextAnalysis [a2db99b7-8b79-58f8-94bf-bbc811eef33d]\n",
      "└ @ Base loading.jl:1260\n",
      "┌ Warning: Package WordTokenizers does not have DataDeps in its dependencies:\n",
      "│ - If you have WordTokenizers checked out for development and have\n",
      "│   added DataDeps as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with WordTokenizers\n",
      "└ Loading DataDeps into WordTokenizers from project dependency, future warnings for WordTokenizers are suppressed.\n",
      "┌ Warning: Package TextAnalysis does not have Requires in its dependencies:\n",
      "│ - If you have TextAnalysis checked out for development and have\n",
      "│   added Requires as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with TextAnalysis\n",
      "└ Loading Requires into TextAnalysis from project dependency, future warnings for TextAnalysis are suppressed.\n",
      "WARNING: Method definition _pullback(Zygote.Context, typeof(Flux.onehot), Any...) in module Flux at /home/iamtejas/.julia/packages/Zygote/z3bQd/src/lib/grad.jl:8 overwritten in module Basic at /home/iamtejas/.julia/packages/Zygote/z3bQd/src/lib/grad.jl:8.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition _pullback(Zygote.Context, typeof(Flux.onehot), Any...) in module Flux at /home/iamtejas/.julia/packages/Zygote/z3bQd/src/lib/grad.jl:8 overwritten in module Basic at /home/iamtejas/.julia/packages/Zygote/z3bQd/src/lib/grad.jl:8.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition get_activation(Any) in module ALBERT at /home/iamtejas/.julia/dev/TextAnalysis/src/albert/tfckpt2bsonforalbert.jl:147 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/albert/pretrain.jl:63.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition _create_classifier() in module ALBERT at /home/iamtejas/.julia/dev/TextAnalysis/src/albert/tfckpt2bsonforalbert.jl:160 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/albert/pretrain.jl:75.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition _create_classifier##kw(Any, typeof(TextAnalysis.ALBERT._create_classifier)) in module ALBERT at /home/iamtejas/.julia/dev/TextAnalysis/src/albert/tfckpt2bsonforalbert.jl:160 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/albert/pretrain.jl:75.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition (::Type{TextAnalysis.MLE})(Any) in module TextAnalysis at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:10 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:22.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition (::Type{TextAnalysis.Lidstone})(Any, Any) in module TextAnalysis at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:32 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:45.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition (::Type{TextAnalysis.Laplace})(Any, Any) in module TextAnalysis at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:62 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:67.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition (::Type{TextAnalysis.WittenBellInterpolated})(Any) in module TextAnalysis at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:131 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:143.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition (::Type{TextAnalysis.KneserNeyInterpolated})(Any, Any) in module TextAnalysis at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:207 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/LM/langmodel.jl:220.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "WARNING: Method definition __init__() in module TextAnalysis at /home/iamtejas/.julia/dev/TextAnalysis/src/TextAnalysis.jl:125 overwritten at /home/iamtejas/.julia/dev/TextAnalysis/src/TextAnalysis.jl:158.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n",
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv /home/iamtejas/.julia/packages/CUDAdrv/aBgcd/src/CUDAdrv.jl:69\n",
      "┌ Warning: The call to compilecache failed to create a usable precompiled cache file for TextAnalysis [a2db99b7-8b79-58f8-94bf-bbc811eef33d]\n",
      "│   exception = ErrorException(\"Required dependency BytePairEncoding [a4280ba5-8788-555a-8ca8-4a8c3d966a71] failed to load from a cache file.\")\n",
      "└ @ Base loading.jl:1041\n",
      "┌ Info: Precompiling WordTokenizers [796a5d58-b03d-544a-977e-18100b691f6e]\n",
      "└ @ Base loading.jl:1260\n",
      "┌ Warning: Package WordTokenizers does not have DataDeps in its dependencies:\n",
      "│ - If you have WordTokenizers checked out for development and have\n",
      "│   added DataDeps as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with WordTokenizers\n",
      "└ Loading DataDeps into WordTokenizers from project dependency, future warnings for WordTokenizers are suppressed.\n",
      "┌ Warning: Package TextAnalysis does not have Requires in its dependencies:\n",
      "│ - If you have TextAnalysis checked out for development and have\n",
      "│   added Requires as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with TextAnalysis\n",
      "│ Loading Requires into TextAnalysis from project dependency, future warnings for TextAnalysis are suppressed.\n",
      "└ @ nothing nothing:909\n",
      "┌ Info: Precompiling Transformers [21ca0261-441d-5938-ace7-c90938fde4d4]\n",
      "└ @ Base loading.jl:1260\n",
      "WARNING: Method definition _pullback(Zygote.Context, typeof(Flux.onehot), Any...) in module Flux at /home/iamtejas/.julia/packages/Zygote/z3bQd/src/lib/grad.jl:8 overwritten in module Basic at /home/iamtejas/.julia/packages/Zygote/z3bQd/src/lib/grad.jl:8.\n",
      "  ** incremental compilation may be fatally broken for this module **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using TextAnalysis\n",
    "using TextAnalysis.ALBERT # it is where our model reside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets checkout the model version avaliable in PretrainedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " TextAnalysis.ALBERT.ALBERT_V1\n",
       " TextAnalysis.ALBERT.ALBERT_V2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtypes(ALBERT.PretrainedTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check different size model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{String,1}:\n",
       " \"albert_base_v1\"\n",
       " \"albert_large_v1\"\n",
       " \"albert_xlarge_v1\"\n",
       " \"albert_xxlarge_v1\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version( TextAnalysis.ALBERT.ALBERT_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward let us look at the following basic steps involved in using any transformer,\n",
    "\n",
    " ### For preprocessing\n",
    "- Tokenize the input data and other input details such as Attention Mask for BERT to not ignore the attention on padded sequences.\n",
    "- Convert tokens to input ID sequences.\n",
    "- Pad the IDs to a fixed length.\n",
    "\n",
    "### For modelling\n",
    "- Load the model and feed in the input ID sequence (Do it batch wise suitably based on the memory available).\n",
    "- Get the output of the last hidden layer\n",
    "- Last hidden layer has the sequence representation embedding at 1th index\n",
    "- These embeddings can be used as the inputs for different machine learning or deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordTokenizer` will handle the Preprocessing part\n",
    "and `TextAnlaysis` will handle Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program has requested access to the data dependency albert_base_v2.\n",
      "which is not currently installed. It can be installed automatically, and you will not see this message again.\n",
      "\n",
      "albert-weights BSON file converted from official weigths-file by google research .\n",
      "Website: https://github.com/google-research/albert\n",
      "Author: Google Research\n",
      "Licence: Apache License 2.0\n",
      "albert base version2 of size ~46 MB download.\n",
      "\n",
      "\n",
      "\n",
      "Do you want to download the dataset from https://drive.google.com/uc?export=download&id=19llahJFvgjQNQ9pzES2XF0R9JdYwuuTk to \"/home/iamtejas/.julia/datadeps/albert_base_v2\"?\n",
      "[y/n]\n",
      "stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Downloading\n",
      "│   source = https://doc-0k-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t1ojiicstr6fuitbtm40kchpmih6v6gs/1596323625000/15884229709856900679/*/19llahJFvgjQNQ9pzES2XF0R9JdYwuuTk?e=download\n",
      "│   dest = /home/iamtejas/.julia/datadeps/albert_base_v2/albert_base_v2.bson\n",
      "│   progress = NaN\n",
      "│   time_taken = 5.03 s\n",
      "│   time_remaining = NaN s\n",
      "│   average_speed = 8.945 MiB/s\n",
      "│   downloaded = 45.000 MiB\n",
      "│   remaining = ∞ B\n",
      "│   total = ∞ B\n",
      "└ @ HTTP /home/iamtejas/.julia/packages/HTTP/BOJmV/src/download.jl:119\n",
      "┌ Info: Downloading\n",
      "│   source = https://doc-0k-3g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/t1ojiicstr6fuitbtm40kchpmih6v6gs/1596323625000/15884229709856900679/*/19llahJFvgjQNQ9pzES2XF0R9JdYwuuTk?e=download\n",
      "│   dest = /home/iamtejas/.julia/datadeps/albert_base_v2/albert_base_v2.bson\n",
      "│   progress = NaN\n",
      "│   time_taken = 5.21 s\n",
      "│   time_remaining = NaN s\n",
      "│   average_speed = 8.809 MiB/s\n",
      "│   downloaded = 45.903 MiB\n",
      "│   remaining = ∞ B\n",
      "│   total = ∞ B\n",
      "└ @ HTTP /home/iamtejas/.julia/packages/HTTP/BOJmV/src/download.jl:119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel{TextAnalysis.ALBERT.albert_transformer}(\n",
       "  embed = CompositeEmbedding(tok = Embed(128), segment = Embed(128), pe = PositionEmbedding(128, max_len=512), postprocessor = Positionwise(LayerNorm(128), Dropout(0))),\n",
       "  transformers = albert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
       "  classifier = \n",
       "    (\n",
       "      pooler => Dense(768, 768, tanh)\n",
       "      masklm => (\n",
       "        transform => Chain(Dense(768, 128, gelu), LayerNorm(128))\n",
       "        output_bias => Array{Float32,1}\n",
       "      )\n",
       "      nextsentence => Chain(Dense(768, 2), logsoftmax)\n",
       "    )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = ALBERT.from_pretrained( \"albert_base_v2\") #here we are using version 1 i.e base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using WordTokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more detail on tokenizer refer the following [blog](https://tejasvaidhyadev.github.io/blog/Hey-Albert) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordTokenizers.SentencePieceModel(Dict(\"▁shots\" => (-11.2373, 7281),\"▁ordered\" => (-9.84973, 1906),\"▁doubtful\" => (-12.7799, 22569),\"▁glancing\" => (-11.6676, 10426),\"▁disrespect\" => (-13.13, 26682),\"▁without\" => (-8.34227, 367),\"▁pol\" => (-10.7694, 4828),\"chem\" => (-12.3713, 17661),\"▁1947,\" => (-11.7544, 11199),\"▁kw\" => (-10.4402, 3511)…), 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = load(ALBERT_V1,1) #because we are using base-version1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use DataLoader avaliable in [`Transformers`](https://github.com/chengchingwen/Transformers.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using QNLI Dataseet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Channel{String}(sz_max:0,sz_curr:1), Channel{String}(sz_max:0,sz_curr:0), Channel{String}(sz_max:0,sz_curr:0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Transformers.Datasets\n",
    "using Transformers.Datasets.GLUE\n",
    "using Transformers.Basic\n",
    "task = GLUE.QNLI()\n",
    "datas = dataset(Train, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"entailment\", \"not_entailment\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: onehotbatch\n",
    "labels = get_labels(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Preprocessing function \n",
    "\n",
    "APIs[WIP] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makesentence(s1, s2) = [\"[CLS]\"; s1; \"[SEP]\"; s2; \"[SEP]\"]\n",
    "function preprocess(training_batch)\n",
    "ids =[]\n",
    "sent = []\n",
    "for i in 1:length(training_batch[1])\n",
    "    sent1 = tokenizer(spm,training_batch[1][i])\n",
    "    sent2 = tokenizer(spm,training_batch[2][i])\n",
    "    id = makesentence(sent1,sent2)\n",
    "    push!(sent, id)\n",
    "    push!(ids,ids_from_tokens(spm,id))\n",
    "end\n",
    "    #print(sent)\n",
    "    mask = getmask(convert(Array{Array{String,1},1}, sent)) #better API underprogress\n",
    "\n",
    "E = Flux.batchseq(ids,1)\n",
    "E = Flux.stack(E,1)\n",
    "length(E) #output embedding matrix\n",
    "segment = fill!(similar(E), 1)\n",
    "    for (i, sent) ∈ enumerate(sent)\n",
    "      j = findfirst(isequal(\"[SEP]\"), sent)\n",
    "      if j !== nothing\n",
    "        @view(segment[j+1:end, i]) .= 2\n",
    "      end\n",
    "end\n",
    "data = (tok = E,segment = segment)\n",
    "labels = get_labels(task)\n",
    "label = onehotbatch(training_batch[3], labels)\n",
    "return(data,label,mask)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer = TransformerModel{TextAnalysis.ALBERT.albert_transformer}(\n",
      "  embed = CompositeEmbedding(tok = Embed(128), segment = Embed(128), pe = PositionEmbedding(128, max_len=512), postprocessor = Positionwise(LayerNorm(128), Dropout(0))),\n",
      "  transformers = albert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
      "  classifier = \n",
      "    (\n",
      "      pooler => Dense(768, 768, tanh)\n",
      "      clf => Chain(Dropout(0.1), Dense(768, 2), logsoftmax)\n",
      "    )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: gradient\n",
    "import Flux.Optimise: update!\n",
    "\n",
    "clf = Flux.Chain(\n",
    "    Flux.Dropout(0.1),\n",
    "    Flux.Dense(768, length(labels)), Flux.logsoftmax\n",
    ")\n",
    "transformer = gpu(\n",
    "  Basic.set_classifier(transformer, \n",
    "    (\n",
    "      pooler = transformer.classifier.pooler,\n",
    "      clf = clf\n",
    "    )\n",
    "  )\n",
    ")\n",
    "@show transformer\n",
    "\n",
    "#define the loss\n",
    "function loss(data, label, mask=nothing)\n",
    "    e = (transformer.embed(data))\n",
    "    t = (transformer.transformers(e))\n",
    "    l = logcrossentropy(label,\n",
    "         clf(\n",
    "            transformer.classifier.pooler(\n",
    "                t[:,1,:]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l = 5.8916574f0\n",
      "l = 2.2995481f0\n",
      "l = 0.56309104f0\n",
      "l = 0.021793587f0\n",
      "l = 0.0004883651f0\n",
      "l = 7.14015f-5\n",
      "l = 6.914115f-6\n",
      "l = 7.5275195f-5\n",
      "l = 2.3841828f-6\n",
      "l = 3.1470263f-5\n",
      "l = 8.940689f-7\n",
      "l = 1.788139f-7\n",
      "l = -0.0f0\n",
      "l = -0.0f0\n",
      "l = -0.0f0\n",
      "l = -0.0f0\n",
      "l = -0.0f0\n",
      "l = -0.0f0\n",
      "l = -0.0f0\n",
      "l = -0.0f0\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: gradient\n",
    "import Flux.Optimise: update!\n",
    "\n",
    "using CuArrays\n",
    "\n",
    "data_batch = get_batch(datas, 2)\n",
    "data_batch, label_batch, mask =(preprocess(data_batch))\n",
    "for i ∈ 1:20 # iteration of 20 cycles over same data to see convergence \n",
    "#data_batch = get_batch(datas, 2)\n",
    "#data_batch, label_batch, mask = preprocess(data_batch)\n",
    "l= loss(data_batch, label_batch, mask)\n",
    "ps = params(transformer)\n",
    "opt = ADAM(1e-4)\n",
    "@show l\n",
    "  grad = gradient(()-> loss(data_batch, label_batch, mask), ps)\n",
    "  update!(opt, ps, grad)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
